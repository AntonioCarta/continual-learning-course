{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data - Sine Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def uniform(shape, min_value, max_value):\n",
    "    return torch.rand(*shape) * (max_value - min_value) + min_value\n",
    "\n",
    "\n",
    "class MetaLoader:\n",
    "    def __init__(self, num_tasks, samples_per_task):\n",
    "        self.num_tasks = num_tasks\n",
    "        self.samples_per_task = samples_per_task\n",
    "\n",
    "    def __iter__(self):\n",
    "        for ep in range(self.num_tasks):\n",
    "            A = 0.1 + torch.rand(1) * (5.0 - 0.1)\n",
    "            p = torch.rand(1) * np.pi\n",
    "            \n",
    "            x_supp = -5.0 + torch.rand(self.samples_per_task) * 10.0\n",
    "            y_supp = A * torch.sin(x_supp - p)\n",
    "\n",
    "            x_query = -5.0 + torch.rand(self.samples_per_task) * 10.0\n",
    "            y_query = A * torch.sin(x_query - p)\n",
    "\n",
    "            yield (x_supp.unsqueeze(1), y_supp.unsqueeze(1)), (x_query.unsqueeze(1), y_query.unsqueeze(1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 1))\n",
    "for (x, y), (w, z) in MetaLoader(num_tasks=2, samples_per_task=100):\n",
    "    plt.scatter(x.tolist(), y.tolist())\n",
    "    plt.scatter(w.tolist(), z.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import vmap, grad, functional_call\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "\n",
    "\n",
    "class MAML():\n",
    "    def __init__(self, model, meta_lr, inner_lr, inner_step, ts_inner_step, iterations, path):\n",
    "        self.path = path # model checkpoint (for saving or load)\n",
    "\n",
    "        self.model = model\n",
    "        self.meta_lr = meta_lr   # beta learning rate in outer loop\n",
    "        self.inner_lr = inner_lr # alfa learning rate in inner loop\n",
    "        self.tr_inner_steps = inner_step # optimization step in inner loop during training\n",
    "        self.ts_inner_steps = ts_inner_step # inner optimization steps during evaluation\n",
    "        self.iterations = iterations  # epochs\n",
    "        self.meta_optimizer = optim.Adam(self.model.parameters(), lr=self.meta_lr)\n",
    "       \n",
    "        # Store performance history\n",
    "        self.best_loss = 100000  # trackng best loss for model checkpointing\n",
    "        self.tr_history = {\"tr_loss\" : [], \"tr_inner_loss\": []}\n",
    "        self.vl_history = {\"vl_loss\" : [], \"vl_inner_loss\": []}\n",
    "    \n",
    "    def _inner_loss(self, params, buffers, x, y):\n",
    "        \"\"\"functional call to compute loss on the support set\"\"\"\n",
    "        y_out = functional_call(self.model, (params, buffers), x)\n",
    "        return F.mse_loss(y_out, y)\n",
    "\n",
    "    def _inner_loop(self, x_support, y_support, x_query, y_query):\n",
    "        \"\"\"Single task inner loop training.\"\"\"\n",
    "        # needed for functional call\n",
    "        params = dict(self.model.named_parameters())\n",
    "        buffers = dict(self.model.named_buffers())\n",
    "\n",
    "        for _ in range(self.tr_inner_steps):  # inner loop optimization\n",
    "            grads = grad(self._inner_loss)(params, buffers, x_support, y_support)\n",
    "            # inner SGD step: params = params - alpha * grad(inner_loss, params)\n",
    "            params = {k: params[k] - g * self.inner_lr for k, g, in grads.items()}\n",
    "\n",
    "        # final losses on support and query set\n",
    "        supp_loss = self._inner_loss(params, buffers, x_support, y_support)\n",
    "        query_loss = self._inner_loss(params, buffers, x_query, y_query)\n",
    "        return supp_loss, query_loss\n",
    "\n",
    "    def train_episode(self, meta_loader):\n",
    "        \"\"\"Each episode we sample tasks from the meta-loader and optimize the initialization\n",
    "        by minimizing the expected query loss w.r.t. the model initialization (outer loop). \n",
    "        The loss that we are minimizing is the avg validation loss over the tasks, computed \n",
    "        on the model obtained by finetuning the initialization for k steps (inner loop).\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.meta_optimizer.zero_grad()  # reset meta-gradients\n",
    "        num_tasks = len(meta_loader)\n",
    "        all_supp_loss, all_query_loss = 0, 0\n",
    "\n",
    "        # we sample training tasks from the meta-loader.\n",
    "        # each task provides support (train) and query (eval) samples\n",
    "        # if you want, you can parallelize this loop with a vmap!\n",
    "        for (x_support, y_support), (x_query, y_query) in meta_loader:           \n",
    "            # inner loop for each task on support loss\n",
    "            res = self._inner_loop(x_support, y_support, x_query, y_query)\n",
    "            all_supp_loss += res[0]  # accumulate supp loss across all tasks\n",
    "            all_query_loss += res[1]  # accumulate query loss across all tasks   \n",
    "\n",
    "        # outer loop on query loss\n",
    "        all_query_loss.sum().backward()  # meta-gradient of the meta losses (avg query loss for tasks in the episode)\n",
    "        self.meta_optimizer.step()\n",
    "\n",
    "        # track total loss and accuracy\n",
    "        all_query_loss = (all_query_loss.detach().sum() / num_tasks).item()\n",
    "        all_supp_loss = (all_supp_loss.detach().sum() / num_tasks).item()\n",
    "        return all_supp_loss, all_query_loss\n",
    "\n",
    "    def evaluation(self, meta_loader):\n",
    "        \"\"\"Evaluate the initialization on meta-loder tasks.\"\"\"\n",
    "        self.model.eval()\n",
    "        supp_losses, query_losses = [], []\n",
    "\n",
    "        for (x_support, y_support), (x_query, y_query) in meta_loader:\n",
    "            new_params = dict(self.model.named_parameters())\n",
    "            buffers = dict(self.model.named_buffers())\n",
    "\n",
    "            # inner loop optimization with functional API\n",
    "            # NOTE: we are training the init even for the evaluation!\n",
    "            for _ in range(self.ts_inner_steps):  \n",
    "                # forward pass with functional API\n",
    "                supp_logits = functional_call(self.model, (new_params, buffers), x_support)\n",
    "                # loss on support set\n",
    "                supp_loss = F.mse_loss(supp_logits, y_support)\n",
    "                # gradients of the support loss\n",
    "                grads = torch.autograd.grad(supp_loss, new_params.values())\n",
    "                # update the params with an SGD step\n",
    "                new_params = {k: new_params[k] - g * self.inner_lr for k, g, in zip(new_params, grads)}\n",
    "\n",
    "            # add support and query loss to history\n",
    "            supp_losses.append(supp_loss.detach())\n",
    "            qry_logits = functional_call(self.model, (new_params, buffers), x_query).detach()\n",
    "            qry_loss = F.mse_loss(qry_logits, y_query, reduction='none')\n",
    "            query_losses.append(qry_loss.detach())\n",
    "\n",
    "        # get mean inner/meta performance over metabatches\n",
    "        supp_losses = torch.mean(torch.stack(supp_losses)).item()\n",
    "        query_losses = torch.cat(query_losses).mean().item()\n",
    "        return supp_losses, query_losses\n",
    "\n",
    "    def fit_and_evaluate(self, train_loader, eval_loader):\n",
    "        \"\"\"The code here is long but it's mostly printing/plotting, and tracking accuracies.\"\"\"\n",
    "        for iteration in range(self.iterations):\n",
    "            tr_inner_loss, tr_loss = self.train_episode(train_loader)\n",
    "            vl_inner_loss, vl_loss = self.evaluation(eval_loader)\n",
    "\n",
    "            # add performance to history\n",
    "            self.tr_history[\"tr_loss\"].append(tr_loss)\n",
    "            self.vl_history[\"vl_loss\"].append(vl_loss)\n",
    "            self.tr_history[\"tr_inner_loss\"].append(tr_inner_loss)\n",
    "            self.vl_history[\"vl_inner_loss\"].append(vl_inner_loss)\n",
    "\n",
    "            # pretty print\n",
    "            if iteration % (self.iterations // 10) == 0 or iteration == self.iterations - 1:\n",
    "                print(\n",
    "                    f'[Epoch {iteration}] | ',\n",
    "                    f'[TR] Meta Loss: {tr_loss:.2f} -',\n",
    "                    f'Inner Loss: {tr_inner_loss:.2f} - |',\n",
    "                    f'[VL] Meta Loss: {vl_loss:.2f} -  ',\n",
    "                    f'Inner Loss: {vl_inner_loss:.2f} - |')\n",
    "\n",
    "                self.model.eval()\n",
    "                (x_support, y_support), _ = next(iter(eval_loader))\n",
    "                new_params = dict(self.model.named_parameters())\n",
    "                buffers = dict(self.model.named_buffers())\n",
    "                # inner loop optimization\n",
    "                for _ in range(self.ts_inner_steps):  \n",
    "                    supp_logits = functional_call(self.model, (new_params, buffers), x_support)\n",
    "                    supp_loss = F.mse_loss(supp_logits, y_support)\n",
    "                    grads = torch.autograd.grad(supp_loss, new_params.values())\n",
    "                    new_params = {k: new_params[k] - g * self.inner_lr for k, g, in zip(new_params, grads)}\n",
    "\n",
    "                y_out = functional_call(self.model, (new_params, buffers), x_support)\n",
    "                plt.figure(figsize=(4, 1))\n",
    "                plt.scatter(x_support.tolist(), y_support.tolist(), label=\"true\")\n",
    "                plt.scatter(x_support.tolist(), y_out.tolist(), label=\"pred\")\n",
    "                plt.legend()\n",
    "\n",
    "            # model checkpointing\n",
    "            if vl_loss <= self.best_loss:  \n",
    "                self.best_loss = vl_loss\n",
    "                torch.save({\n",
    "                    'epoch': iteration,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.meta_optimizer.state_dict(),\n",
    "                    'loss': vl_loss,\n",
    "                }, self.path)\n",
    "\n",
    "        return self.tr_history, self.vl_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m      4\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(exp_root, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m40\u001b[39m),\n\u001b[0;32m      8\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m maml \u001b[38;5;241m=\u001b[39m \u001b[43mMAML\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mts_inner_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     24\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m MetaLoader(num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, samples_per_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m     27\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m MetaLoader(num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, samples_per_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mMAML.__init__\u001b[1;34m(self, model, meta_lr, inner_lr, inner_step, ts_inner_step, iterations, path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts_inner_steps \u001b[38;5;241m=\u001b[39m ts_inner_step \u001b[38;5;66;03m# inner optimization steps during evaluation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations \u001b[38;5;241m=\u001b[39m iterations  \u001b[38;5;66;03m# epochs\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_lr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Store performance history\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m  \u001b[38;5;66;03m# trackng best loss for model checkpointing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\optim\\optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[1;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\_dynamo\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\_dynamo\\allowed_functions.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_compiling\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashable, is_safe_constant, NP_SUPPORTED_MODULES\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mA note on allowed functions:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03mskipfiles\" there.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFunctionIdSet\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\_dynamo\\utils.py:89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_shapes\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fx\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_python_dispatcher\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\fx\\experimental\\symbolic_shapes.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_guards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ShapeGuard, Source, TracingContext\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloorDiv, Mod, IsNonOverlappingAndDenseIndicator\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msolve\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_solve\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalue_ranges\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bound_sympy, SymPyValueRangeAnalysis, ValueRanges, ValueRangeError\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\torch\\utils\\_sympy\\functions.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m S\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fuzzy_and, fuzzy_not, fuzzy_or\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\sympy\\__init__.py:74\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (to_cnf, to_dnf, to_nnf, And, Or, Not, Xor, Nand, Nor,\n\u001b[0;32m     68\u001b[0m         Implies, Equivalent, ITE, POSform, SOPform, simplify_logic, bool_map,\n\u001b[0;32m     69\u001b[0m         true, false, satisfiable)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01massumptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AppliedPredicate, Predicate, AssumptionsContext,\n\u001b[0;32m     72\u001b[0m         assuming, Q, ask, register_handler, remove_handler, refine)\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n\u001b[0;32m     75\u001b[0m         degree, total_degree, degree_list, LC, LM, LT, pdiv, prem, pquo,\n\u001b[0;32m     76\u001b[0m         pexquo, div, rem, quo, exquo, half_gcdex, gcdex, invert,\n\u001b[0;32m     77\u001b[0m         subresultants, resultant, discriminant, cofactors, gcd_list, gcd,\n\u001b[0;32m     78\u001b[0m         lcm_list, lcm, terms_gcd, trunc, monic, content, primitive, compose,\n\u001b[0;32m     79\u001b[0m         decompose, sturm, gff_list, gff, sqf_norm, sqf_part, sqf_list, sqf,\n\u001b[0;32m     80\u001b[0m         factor_list, factor, intervals, refine_root, count_roots, real_roots,\n\u001b[0;32m     81\u001b[0m         nroots, ground_roots, nth_power_roots_poly, cancel, reduced, groebner,\n\u001b[0;32m     82\u001b[0m         is_zero_dimensional, GroebnerBasis, poly, symmetrize, horner,\n\u001b[0;32m     83\u001b[0m         interpolate, rational_interpolate, viete, together,\n\u001b[0;32m     84\u001b[0m         BasePolynomialError, ExactQuotientFailed, PolynomialDivisionFailed,\n\u001b[0;32m     85\u001b[0m         OperationNotSupported, HeuristicGCDFailed, HomomorphismFailed,\n\u001b[0;32m     86\u001b[0m         IsomorphismFailed, ExtraneousFactors, EvaluationFailed,\n\u001b[0;32m     87\u001b[0m         RefinementFailed, CoercionFailed, NotInvertible, NotReversible,\n\u001b[0;32m     88\u001b[0m         NotAlgebraic, DomainError, PolynomialError, UnificationFailed,\n\u001b[0;32m     89\u001b[0m         GeneratorsError, GeneratorsNeeded, ComputationFailed,\n\u001b[0;32m     90\u001b[0m         UnivariatePolynomialError, MultivariatePolynomialError,\n\u001b[0;32m     91\u001b[0m         PolificationFailed, OptionError, FlagError, minpoly,\n\u001b[0;32m     92\u001b[0m         minimal_polynomial, primitive_element, field_isomorphism,\n\u001b[0;32m     93\u001b[0m         to_number_field, isolate, round_two, prime_decomp, prime_valuation,\n\u001b[0;32m     94\u001b[0m         galois_group, itermonomials, Monomial, lex, grlex,\n\u001b[0;32m     95\u001b[0m         grevlex, ilex, igrlex, igrevlex, CRootOf, rootof, RootOf,\n\u001b[0;32m     96\u001b[0m         ComplexRootOf, RootSum, roots, Domain, FiniteField, IntegerRing,\n\u001b[0;32m     97\u001b[0m         RationalField, RealField, ComplexField, PythonFiniteField,\n\u001b[0;32m     98\u001b[0m         GMPYFiniteField, PythonIntegerRing, GMPYIntegerRing, PythonRational,\n\u001b[0;32m     99\u001b[0m         GMPYRationalField, AlgebraicField, PolynomialRing, FractionField,\n\u001b[0;32m    100\u001b[0m         ExpressionDomain, FF_python, FF_gmpy, ZZ_python, ZZ_gmpy, QQ_python,\n\u001b[0;32m    101\u001b[0m         QQ_gmpy, GF, FF, ZZ, QQ, ZZ_I, QQ_I, RR, CC, EX, EXRAW,\n\u001b[0;32m    102\u001b[0m         construct_domain, swinnerton_dyer_poly, cyclotomic_poly,\n\u001b[0;32m    103\u001b[0m         symmetric_poly, random_poly, interpolating_poly, jacobi_poly,\n\u001b[0;32m    104\u001b[0m         chebyshevt_poly, chebyshevu_poly, hermite_poly, hermite_prob_poly,\n\u001b[0;32m    105\u001b[0m         legendre_poly, laguerre_poly, apart, apart_list, assemble_partfrac_list,\n\u001b[0;32m    106\u001b[0m         Options, ring, xring, vring, sring, field, xfield, vfield, sfield)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Order, O, limit, Limit, gruntz, series, approximants,\n\u001b[0;32m    109\u001b[0m         residue, EmptySequence, SeqPer, SeqFormula, sequence, SeqAdd, SeqMul,\n\u001b[0;32m    110\u001b[0m         fourier_series, fps, difference_delta, limit_seq)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (factorial, factorial2, rf, ff, binomial,\n\u001b[0;32m    113\u001b[0m         RisingFactorial, FallingFactorial, subfactorial, carmichael,\n\u001b[0;32m    114\u001b[0m         fibonacci, lucas, motzkin, tribonacci, harmonic, bernoulli, bell, euler,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m         Znm, elliptic_k, elliptic_f, elliptic_e, elliptic_pi, beta, mathieus,\n\u001b[0;32m    134\u001b[0m         mathieuc, mathieusprime, mathieucprime, riemann_xi, betainc, betainc_regularized)\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\sympy\\polys\\__init__.py:78\u001b[0m\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPurePoly\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpoly_from_expr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparallel_poly_from_expr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_degree\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree_list\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpquo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvfield\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfield\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     66\u001b[0m ]\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolytools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (Poly, PurePoly, poly_from_expr,\n\u001b[0;32m     69\u001b[0m         parallel_poly_from_expr, degree, total_degree, degree_list, LC, LM,\n\u001b[0;32m     70\u001b[0m         LT, pdiv, prem, pquo, pexquo, div, rem, quo, exquo, half_gcdex, gcdex,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m         count_roots, real_roots, nroots, ground_roots, nth_power_roots_poly,\n\u001b[0;32m     76\u001b[0m         cancel, reduced, groebner, is_zero_dimensional, GroebnerBasis, poly)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfuncs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (symmetrize, horner, interpolate,\n\u001b[0;32m     79\u001b[0m         rational_interpolate, viete)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrationaltools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m together\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyerrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (BasePolynomialError, ExactQuotientFailed,\n\u001b[0;32m     84\u001b[0m         PolynomialDivisionFailed, OperationNotSupported, HeuristicGCDFailed,\n\u001b[0;32m     85\u001b[0m         HomomorphismFailed, IsomorphismFailed, ExtraneousFactors,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m         MultivariatePolynomialError, PolificationFailed, OptionError,\n\u001b[0;32m     91\u001b[0m         FlagError)\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\sympy\\polys\\polyfuncs.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyoptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_flags, build_options\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolytools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m poly_from_expr, Poly\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecialpolys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     symmetric_poly, interpolating_poly)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sring\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m numbered_symbols, take, public\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\sympy\\polys\\specialpolys.py:298\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dmp_mul(f, h, n, K), dmp_mul(g, h, n, K), h\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# A few useful polynomials from Wang's paper ('78).\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ring\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_f_0\u001b[39m():\n\u001b[0;32m    301\u001b[0m     R, x, y, z \u001b[38;5;241m=\u001b[39m ring(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx,y,z\u001b[39m\u001b[38;5;124m\"\u001b[39m, ZZ)\n",
      "File \u001b[1;32mc:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\sympy\\polys\\rings.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msympify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CantSympify, sympify\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mntheory\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultinomial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multinomial_coefficients\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompatibility\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IPolys\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstructor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m construct_domain\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolys\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensebasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dmp_to_dict, dmp_from_dict\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[1;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "data_root = './logs'\n",
    "exp_root = './logs'\n",
    "os.makedirs(exp_root, exist_ok=True)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 40),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(40, 40),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(40, 40),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(40, 1)\n",
    ").to('cpu')\n",
    "\n",
    "maml = MAML(\n",
    "    model=model,\n",
    "    meta_lr=1e-3, \n",
    "    inner_lr=0.01, \n",
    "    inner_step=10, \n",
    "    ts_inner_step=10, \n",
    "    iterations=400, \n",
    "    path=data_root + \"/model.pth\"\n",
    ")\n",
    "\n",
    "train_loader = MetaLoader(num_tasks=10, samples_per_task=25)\n",
    "test_loader = MetaLoader(num_tasks=10, samples_per_task=25)\n",
    "tr_history, vl_history = maml.fit_and_evaluate(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Graph\n",
    "\n",
    "It's big and messy, but you can see that there is the whole computational graph in `all_query_loss`. Going from the leaf () towards the roots:\n",
    "- We have two MSELossBackward: these are the losses computed on the finetuned model on the query sets\n",
    "- The SGD updates for each task (SubBackward0): We have two because each task trained a different model\n",
    "- The MSELossBackwardBackward: these are the inner loss computed on the support set for each task\n",
    "\n",
    "Try to experiment with it!\n",
    "- if you change the the number of SGD steps in the inner loop (`inner_steps`) you see that the computational graph grows deeper\n",
    "    - each SGD step adds operations to the computational graph\n",
    "- if you add more tasks (`num_tasks` in the `MetaLoader`) the graph grows \"wider\": each task trains a separate model, starting from the same init (the blue rectangles are the model's initialization parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_root' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# torch.nn.ReLU(),\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# torch.nn.Linear(40, 1)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      9\u001b[0m maml \u001b[38;5;241m=\u001b[39m MAML(\n\u001b[0;32m     10\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     11\u001b[0m     meta_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, \n\u001b[0;32m     12\u001b[0m     inner_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, \n\u001b[0;32m     13\u001b[0m     inner_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m     14\u001b[0m     ts_inner_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[0;32m     15\u001b[0m     iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m, \n\u001b[1;32m---> 16\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[43mdata_root\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m MetaLoader(num_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, samples_per_task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     20\u001b[0m maml\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_root' is not defined"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 1),\n",
    "    # torch.nn.ReLU(),\n",
    "    # torch.nn.Linear(40, 1)\n",
    ")\n",
    "\n",
    "maml = MAML(\n",
    "    model=model,\n",
    "    meta_lr=1e-3, \n",
    "    inner_lr=0.01, \n",
    "    inner_step=2, \n",
    "    ts_inner_step=1, \n",
    "    iterations=400, \n",
    "    path=data_root + \"/model.pth\"\n",
    ")\n",
    "\n",
    "train_loader = MetaLoader(num_tasks=1, samples_per_task=3)\n",
    "maml.model.train()\n",
    "num_tasks = len(train_loader)\n",
    "all_supp_loss, all_query_loss = 0, 0\n",
    "\n",
    "for (x_support, y_support), (x_query, y_query) in train_loader:           \n",
    "    # we are calling the private method of the class _inner_loop, \n",
    "    # which is something that you should avoid in general.\n",
    "    # I'm doing it here only because it's the quickest way to create the torchviz plot\n",
    "\n",
    "    # inner loop for each task on support loss\n",
    "    res = maml._inner_loop(x_support, y_support, x_query, y_query)\n",
    "    all_supp_loss += res[0]\n",
    "    all_query_loss += res[1]   \n",
    "\n",
    "make_dot(all_query_loss.sum(), params=dict(maml.model.named_parameters()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "if you studied the implementation above, understanding, and now you feel adventurous and want to test your knowledge, you can try to:\n",
    "- implement first order MAML by truncating the gradient. You can use `torchviz` to debug your code. Plot the computational graph and check that the gradient is truncated.\n",
    "- add momentum to the optimization step\n",
    "- you can implement some of the MAML improvements in Antoniou, Antreas, Harrison Edwards, and Amos Storkey. \"How to train your MAML.\"arXiv preprint arXiv:1810.09502(2018).\n",
    "    - some are quick and easy to implement, others may require more work\n",
    " \n",
    "Keep in mind that these exercises are much more difficult than the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "791aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
