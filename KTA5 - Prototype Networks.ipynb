{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2821b1a5",
   "metadata": {},
   "source": [
    "# Prototype Networks\n",
    "implementation based on [orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch](https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e33bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install avalanche-lib==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3241044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules import Module\n",
    "from types import SimpleNamespace\n",
    "from utils import load_omniglot_data, init_seed\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe8bcc",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "\n",
    "Don't forget to increase epochs and iterations and to set `cuda` if you want to get the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "010b59e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = SimpleNamespace(\n",
    "    # folders\n",
    "    dataset_root='.' + os.sep + 'dataset',  # path to dataset\n",
    "    experiment_root='.' + os.sep + 'output',  # root where to store models, losses and accuracies\n",
    "    \n",
    "    # training hparams\n",
    "    epochs=1,  # number of epochs to train for, default=100\n",
    "    learning_rate=0.001,  # learning rate for the model\n",
    "    lr_scheduler_step=20,  # StepLR learning rate scheduler step\n",
    "    lr_scheduler_gamma=0.5,  # StepLR learning rate scheduler gamma\n",
    "    iterations=10,  # number of episodes per epoch, default=100\n",
    "    classes_per_it_tr=60,  # number of random classes per episode for training\n",
    "    manual_seed=7,  # input for the manual seeds initializations\n",
    "    cuda=False,\n",
    "    \n",
    "    # task hparams\n",
    "    num_support_tr=5,  # number of samples per class to use as support for training\n",
    "    num_query_tr=5,  # number of samples per class to use as query for training\n",
    "    classes_per_it_val=5,  # number of random classes per episode for validation\n",
    "    num_support_val=5,  # number of samples per class to use as support for validation\n",
    "    num_query_val=15  # number of samples per class to use as query for validation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65166351",
   "metadata": {},
   "source": [
    "# Omniglot Data\n",
    "\n",
    "![img](https://raw.githubusercontent.com/brendenlake/omniglot/master/omniglot_grid.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce835c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Dataset: Found 82240 items \n",
      "== Dataset: Found 4112 classes\n",
      "== Dataset: Found 33840 items \n",
      "== Dataset: Found 1692 classes\n"
     ]
    }
   ],
   "source": [
    "train_data = load_omniglot_data(options, 'train')\n",
    "test_data = load_omniglot_data(options, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3a380",
   "metadata": {},
   "source": [
    "notice that train and test splits have different alphabets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36ccc051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tagalog\\\\character01\\\\rot000',\n",
       " 'Tagalog\\\\character01\\\\rot090',\n",
       " 'Tagalog\\\\character01\\\\rot180',\n",
       " 'Tagalog\\\\character01\\\\rot270',\n",
       " 'Tagalog\\\\character02\\\\rot000',\n",
       " 'Tagalog\\\\character02\\\\rot090',\n",
       " 'Tagalog\\\\character02\\\\rot180',\n",
       " 'Tagalog\\\\character02\\\\rot270',\n",
       " 'Tagalog\\\\character03\\\\rot000',\n",
       " 'Tagalog\\\\character03\\\\rot090',\n",
       " 'Tagalog\\\\character03\\\\rot180',\n",
       " 'Tagalog\\\\character03\\\\rot270',\n",
       " 'Tagalog\\\\character04\\\\rot000',\n",
       " 'Tagalog\\\\character04\\\\rot090',\n",
       " 'Tagalog\\\\character04\\\\rot180',\n",
       " 'Tagalog\\\\character04\\\\rot270',\n",
       " 'Tagalog\\\\character05\\\\rot000',\n",
       " 'Tagalog\\\\character05\\\\rot090',\n",
       " 'Tagalog\\\\character05\\\\rot180',\n",
       " 'Tagalog\\\\character05\\\\rot270',\n",
       " 'Tagalog\\\\character06\\\\rot000',\n",
       " 'Tagalog\\\\character06\\\\rot090',\n",
       " 'Tagalog\\\\character06\\\\rot180',\n",
       " 'Tagalog\\\\character06\\\\rot270',\n",
       " 'Tagalog\\\\character07\\\\rot000',\n",
       " 'Tagalog\\\\character07\\\\rot090',\n",
       " 'Tagalog\\\\character07\\\\rot180',\n",
       " 'Tagalog\\\\character07\\\\rot270',\n",
       " 'Tagalog\\\\character08\\\\rot000',\n",
       " 'Tagalog\\\\character08\\\\rot090',\n",
       " 'Tagalog\\\\character08\\\\rot180',\n",
       " 'Tagalog\\\\character08\\\\rot270',\n",
       " 'Tagalog\\\\character09\\\\rot000',\n",
       " 'Tagalog\\\\character09\\\\rot090',\n",
       " 'Tagalog\\\\character09\\\\rot180',\n",
       " 'Tagalog\\\\character09\\\\rot270',\n",
       " 'Tagalog\\\\character10\\\\rot000',\n",
       " 'Tagalog\\\\character10\\\\rot090',\n",
       " 'Tagalog\\\\character10\\\\rot180',\n",
       " 'Tagalog\\\\character10\\\\rot270',\n",
       " 'Tagalog\\\\character11\\\\rot000',\n",
       " 'Tagalog\\\\character11\\\\rot090',\n",
       " 'Tagalog\\\\character11\\\\rot180',\n",
       " 'Tagalog\\\\character11\\\\rot270',\n",
       " 'Tagalog\\\\character12\\\\rot000',\n",
       " 'Tagalog\\\\character12\\\\rot090',\n",
       " 'Tagalog\\\\character12\\\\rot180',\n",
       " 'Tagalog\\\\character12\\\\rot270',\n",
       " 'Tagalog\\\\character13\\\\rot000',\n",
       " 'Tagalog\\\\character13\\\\rot090',\n",
       " 'Tagalog\\\\character13\\\\rot180',\n",
       " 'Tagalog\\\\character13\\\\rot270',\n",
       " 'Tagalog\\\\character14\\\\rot000',\n",
       " 'Tagalog\\\\character14\\\\rot090',\n",
       " 'Tagalog\\\\character14\\\\rot180',\n",
       " 'Tagalog\\\\character14\\\\rot270',\n",
       " 'Tagalog\\\\character15\\\\rot000',\n",
       " 'Tagalog\\\\character15\\\\rot090',\n",
       " 'Tagalog\\\\character15\\\\rot180',\n",
       " 'Tagalog\\\\character15\\\\rot270',\n",
       " 'Tagalog\\\\character16\\\\rot000',\n",
       " 'Tagalog\\\\character16\\\\rot090',\n",
       " 'Tagalog\\\\character16\\\\rot180',\n",
       " 'Tagalog\\\\character16\\\\rot270',\n",
       " 'Tagalog\\\\character17\\\\rot000',\n",
       " 'Tagalog\\\\character17\\\\rot090',\n",
       " 'Tagalog\\\\character17\\\\rot180',\n",
       " 'Tagalog\\\\character17\\\\rot270',\n",
       " 'Tifinagh\\\\character01\\\\rot000',\n",
       " 'Tifinagh\\\\character01\\\\rot090',\n",
       " 'Tifinagh\\\\character01\\\\rot180',\n",
       " 'Tifinagh\\\\character01\\\\rot270',\n",
       " 'Tifinagh\\\\character02\\\\rot000',\n",
       " 'Tifinagh\\\\character02\\\\rot090',\n",
       " 'Tifinagh\\\\character02\\\\rot180',\n",
       " 'Tifinagh\\\\character02\\\\rot270',\n",
       " 'Tifinagh\\\\character03\\\\rot000',\n",
       " 'Tifinagh\\\\character03\\\\rot090',\n",
       " 'Tifinagh\\\\character03\\\\rot180',\n",
       " 'Tifinagh\\\\character03\\\\rot270',\n",
       " 'Tifinagh\\\\character04\\\\rot000',\n",
       " 'Tifinagh\\\\character04\\\\rot090',\n",
       " 'Tifinagh\\\\character04\\\\rot180',\n",
       " 'Tifinagh\\\\character04\\\\rot270',\n",
       " 'Tifinagh\\\\character05\\\\rot000',\n",
       " 'Tifinagh\\\\character05\\\\rot090',\n",
       " 'Tifinagh\\\\character05\\\\rot180',\n",
       " 'Tifinagh\\\\character05\\\\rot270',\n",
       " 'Tifinagh\\\\character06\\\\rot000',\n",
       " 'Tifinagh\\\\character06\\\\rot090',\n",
       " 'Tifinagh\\\\character06\\\\rot180',\n",
       " 'Tifinagh\\\\character06\\\\rot270',\n",
       " 'Tifinagh\\\\character07\\\\rot000',\n",
       " 'Tifinagh\\\\character07\\\\rot090',\n",
       " 'Tifinagh\\\\character07\\\\rot180',\n",
       " 'Tifinagh\\\\character07\\\\rot270',\n",
       " 'Tifinagh\\\\character08\\\\rot000',\n",
       " 'Tifinagh\\\\character08\\\\rot090',\n",
       " 'Tifinagh\\\\character08\\\\rot180',\n",
       " 'Tifinagh\\\\character08\\\\rot270',\n",
       " 'Tifinagh\\\\character09\\\\rot000',\n",
       " 'Tifinagh\\\\character09\\\\rot090',\n",
       " 'Tifinagh\\\\character09\\\\rot180',\n",
       " 'Tifinagh\\\\character09\\\\rot270',\n",
       " 'Tifinagh\\\\character10\\\\rot000',\n",
       " 'Tifinagh\\\\character10\\\\rot090',\n",
       " 'Tifinagh\\\\character10\\\\rot180',\n",
       " 'Tifinagh\\\\character10\\\\rot270',\n",
       " 'Tifinagh\\\\character11\\\\rot000',\n",
       " 'Tifinagh\\\\character11\\\\rot090',\n",
       " 'Tifinagh\\\\character11\\\\rot180',\n",
       " 'Tifinagh\\\\character11\\\\rot270',\n",
       " 'Tifinagh\\\\character12\\\\rot000',\n",
       " 'Tifinagh\\\\character12\\\\rot090',\n",
       " 'Tifinagh\\\\character12\\\\rot180',\n",
       " 'Tifinagh\\\\character12\\\\rot270',\n",
       " 'Tifinagh\\\\character13\\\\rot000',\n",
       " 'Tifinagh\\\\character13\\\\rot090',\n",
       " 'Tifinagh\\\\character13\\\\rot180',\n",
       " 'Tifinagh\\\\character13\\\\rot270',\n",
       " 'Tifinagh\\\\character14\\\\rot000',\n",
       " 'Tifinagh\\\\character14\\\\rot090',\n",
       " 'Tifinagh\\\\character14\\\\rot180',\n",
       " 'Tifinagh\\\\character14\\\\rot270',\n",
       " 'Tifinagh\\\\character15\\\\rot000',\n",
       " 'Tifinagh\\\\character15\\\\rot090',\n",
       " 'Tifinagh\\\\character15\\\\rot180',\n",
       " 'Tifinagh\\\\character15\\\\rot270',\n",
       " 'Tifinagh\\\\character16\\\\rot000',\n",
       " 'Tifinagh\\\\character16\\\\rot090',\n",
       " 'Tifinagh\\\\character16\\\\rot180',\n",
       " 'Tifinagh\\\\character16\\\\rot270',\n",
       " 'Tifinagh\\\\character17\\\\rot000',\n",
       " 'Tifinagh\\\\character17\\\\rot090',\n",
       " 'Tifinagh\\\\character17\\\\rot180',\n",
       " 'Tifinagh\\\\character17\\\\rot270',\n",
       " 'Tifinagh\\\\character18\\\\rot000',\n",
       " 'Tifinagh\\\\character18\\\\rot090',\n",
       " 'Tifinagh\\\\character18\\\\rot180',\n",
       " 'Tifinagh\\\\character18\\\\rot270',\n",
       " 'Tifinagh\\\\character19\\\\rot000',\n",
       " 'Tifinagh\\\\character19\\\\rot090',\n",
       " 'Tifinagh\\\\character19\\\\rot180',\n",
       " 'Tifinagh\\\\character19\\\\rot270',\n",
       " 'Tifinagh\\\\character20\\\\rot000',\n",
       " 'Tifinagh\\\\character20\\\\rot090',\n",
       " 'Tifinagh\\\\character20\\\\rot180',\n",
       " 'Tifinagh\\\\character20\\\\rot270',\n",
       " 'Tifinagh\\\\character21\\\\rot000',\n",
       " 'Tifinagh\\\\character21\\\\rot090',\n",
       " 'Tifinagh\\\\character21\\\\rot180',\n",
       " 'Tifinagh\\\\character21\\\\rot270',\n",
       " 'Tifinagh\\\\character22\\\\rot000',\n",
       " 'Tifinagh\\\\character22\\\\rot090',\n",
       " 'Tifinagh\\\\character22\\\\rot180',\n",
       " 'Tifinagh\\\\character22\\\\rot270',\n",
       " 'Tifinagh\\\\character23\\\\rot000',\n",
       " 'Tifinagh\\\\character23\\\\rot090',\n",
       " 'Tifinagh\\\\character23\\\\rot180',\n",
       " 'Tifinagh\\\\character23\\\\rot270',\n",
       " 'Tifinagh\\\\character24\\\\rot000',\n",
       " 'Tifinagh\\\\character24\\\\rot090',\n",
       " 'Tifinagh\\\\character24\\\\rot180',\n",
       " 'Tifinagh\\\\character24\\\\rot270',\n",
       " 'Tifinagh\\\\character25\\\\rot000',\n",
       " 'Tifinagh\\\\character25\\\\rot090',\n",
       " 'Tifinagh\\\\character25\\\\rot180',\n",
       " 'Tifinagh\\\\character25\\\\rot270',\n",
       " 'Tifinagh\\\\character26\\\\rot000',\n",
       " 'Tifinagh\\\\character26\\\\rot090',\n",
       " 'Tifinagh\\\\character26\\\\rot180',\n",
       " 'Tifinagh\\\\character26\\\\rot270',\n",
       " 'Tifinagh\\\\character27\\\\rot000',\n",
       " 'Tifinagh\\\\character27\\\\rot090',\n",
       " 'Tifinagh\\\\character27\\\\rot180',\n",
       " 'Tifinagh\\\\character27\\\\rot270',\n",
       " 'Tifinagh\\\\character28\\\\rot000',\n",
       " 'Tifinagh\\\\character28\\\\rot090',\n",
       " 'Tifinagh\\\\character28\\\\rot180',\n",
       " 'Tifinagh\\\\character28\\\\rot270',\n",
       " 'Tifinagh\\\\character29\\\\rot000',\n",
       " 'Tifinagh\\\\character29\\\\rot090',\n",
       " 'Tifinagh\\\\character29\\\\rot180',\n",
       " 'Tifinagh\\\\character29\\\\rot270',\n",
       " 'Tifinagh\\\\character30\\\\rot000',\n",
       " 'Tifinagh\\\\character30\\\\rot090',\n",
       " 'Tifinagh\\\\character30\\\\rot180',\n",
       " 'Tifinagh\\\\character30\\\\rot270',\n",
       " 'Tifinagh\\\\character31\\\\rot000',\n",
       " 'Tifinagh\\\\character31\\\\rot090',\n",
       " 'Tifinagh\\\\character31\\\\rot180',\n",
       " 'Tifinagh\\\\character31\\\\rot270',\n",
       " 'Tifinagh\\\\character32\\\\rot000',\n",
       " 'Tifinagh\\\\character32\\\\rot090',\n",
       " 'Tifinagh\\\\character32\\\\rot180',\n",
       " 'Tifinagh\\\\character32\\\\rot270',\n",
       " 'Tifinagh\\\\character33\\\\rot000',\n",
       " 'Tifinagh\\\\character33\\\\rot090',\n",
       " 'Tifinagh\\\\character33\\\\rot180',\n",
       " 'Tifinagh\\\\character33\\\\rot270',\n",
       " 'Tifinagh\\\\character34\\\\rot000',\n",
       " 'Tifinagh\\\\character34\\\\rot090',\n",
       " 'Tifinagh\\\\character34\\\\rot180',\n",
       " 'Tifinagh\\\\character34\\\\rot270',\n",
       " 'Tifinagh\\\\character35\\\\rot000',\n",
       " 'Tifinagh\\\\character35\\\\rot090',\n",
       " 'Tifinagh\\\\character35\\\\rot180',\n",
       " 'Tifinagh\\\\character35\\\\rot270',\n",
       " 'Tifinagh\\\\character36\\\\rot000',\n",
       " 'Tifinagh\\\\character36\\\\rot090',\n",
       " 'Tifinagh\\\\character36\\\\rot180',\n",
       " 'Tifinagh\\\\character36\\\\rot270',\n",
       " 'Tifinagh\\\\character37\\\\rot000',\n",
       " 'Tifinagh\\\\character37\\\\rot090',\n",
       " 'Tifinagh\\\\character37\\\\rot180',\n",
       " 'Tifinagh\\\\character37\\\\rot270',\n",
       " 'Tifinagh\\\\character38\\\\rot000',\n",
       " 'Tifinagh\\\\character38\\\\rot090',\n",
       " 'Tifinagh\\\\character38\\\\rot180',\n",
       " 'Tifinagh\\\\character38\\\\rot270',\n",
       " 'Tifinagh\\\\character39\\\\rot000',\n",
       " 'Tifinagh\\\\character39\\\\rot090',\n",
       " 'Tifinagh\\\\character39\\\\rot180',\n",
       " 'Tifinagh\\\\character39\\\\rot270',\n",
       " 'Tifinagh\\\\character40\\\\rot000',\n",
       " 'Tifinagh\\\\character40\\\\rot090',\n",
       " 'Tifinagh\\\\character40\\\\rot180',\n",
       " 'Tifinagh\\\\character40\\\\rot270',\n",
       " 'Tifinagh\\\\character41\\\\rot000',\n",
       " 'Tifinagh\\\\character41\\\\rot090',\n",
       " 'Tifinagh\\\\character41\\\\rot180',\n",
       " 'Tifinagh\\\\character41\\\\rot270',\n",
       " 'Tifinagh\\\\character42\\\\rot000',\n",
       " 'Tifinagh\\\\character42\\\\rot090',\n",
       " 'Tifinagh\\\\character42\\\\rot180',\n",
       " 'Tifinagh\\\\character42\\\\rot270',\n",
       " 'Tifinagh\\\\character43\\\\rot000',\n",
       " 'Tifinagh\\\\character43\\\\rot090',\n",
       " 'Tifinagh\\\\character43\\\\rot180',\n",
       " 'Tifinagh\\\\character43\\\\rot270',\n",
       " 'Tifinagh\\\\character44\\\\rot000',\n",
       " 'Tifinagh\\\\character44\\\\rot090',\n",
       " 'Tifinagh\\\\character44\\\\rot180',\n",
       " 'Tifinagh\\\\character44\\\\rot270',\n",
       " 'Tifinagh\\\\character45\\\\rot000',\n",
       " 'Tifinagh\\\\character45\\\\rot090',\n",
       " 'Tifinagh\\\\character45\\\\rot180',\n",
       " 'Tifinagh\\\\character45\\\\rot270',\n",
       " 'Tifinagh\\\\character46\\\\rot000',\n",
       " 'Tifinagh\\\\character46\\\\rot090',\n",
       " 'Tifinagh\\\\character46\\\\rot180',\n",
       " 'Tifinagh\\\\character46\\\\rot270',\n",
       " 'Tifinagh\\\\character47\\\\rot000',\n",
       " 'Tifinagh\\\\character47\\\\rot090',\n",
       " 'Tifinagh\\\\character47\\\\rot180',\n",
       " 'Tifinagh\\\\character47\\\\rot270',\n",
       " 'Tifinagh\\\\character48\\\\rot000',\n",
       " 'Tifinagh\\\\character48\\\\rot090',\n",
       " 'Tifinagh\\\\character48\\\\rot180',\n",
       " 'Tifinagh\\\\character48\\\\rot270',\n",
       " 'Tifinagh\\\\character49\\\\rot000',\n",
       " 'Tifinagh\\\\character49\\\\rot090',\n",
       " 'Tifinagh\\\\character49\\\\rot180',\n",
       " 'Tifinagh\\\\character49\\\\rot270',\n",
       " 'Tifinagh\\\\character50\\\\rot000',\n",
       " 'Tifinagh\\\\character50\\\\rot090',\n",
       " 'Tifinagh\\\\character50\\\\rot180',\n",
       " 'Tifinagh\\\\character50\\\\rot270',\n",
       " 'Tifinagh\\\\character51\\\\rot000',\n",
       " 'Tifinagh\\\\character51\\\\rot090',\n",
       " 'Tifinagh\\\\character51\\\\rot180',\n",
       " 'Tifinagh\\\\character51\\\\rot270',\n",
       " 'Tifinagh\\\\character52\\\\rot000',\n",
       " 'Tifinagh\\\\character52\\\\rot090',\n",
       " 'Tifinagh\\\\character52\\\\rot180',\n",
       " 'Tifinagh\\\\character52\\\\rot270',\n",
       " 'Tifinagh\\\\character53\\\\rot000',\n",
       " 'Tifinagh\\\\character53\\\\rot090',\n",
       " 'Tifinagh\\\\character53\\\\rot180',\n",
       " 'Tifinagh\\\\character53\\\\rot270',\n",
       " 'Tifinagh\\\\character54\\\\rot000',\n",
       " 'Tifinagh\\\\character54\\\\rot090',\n",
       " 'Tifinagh\\\\character54\\\\rot180',\n",
       " 'Tifinagh\\\\character54\\\\rot270',\n",
       " 'Tifinagh\\\\character55\\\\rot000',\n",
       " 'Tifinagh\\\\character55\\\\rot090',\n",
       " 'Tifinagh\\\\character55\\\\rot180',\n",
       " 'Tifinagh\\\\character55\\\\rot270']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda s: s.startswith('T'), train_data.idx_classes.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c80378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tengwar\\\\character01\\\\rot000',\n",
       " 'Tengwar\\\\character01\\\\rot090',\n",
       " 'Tengwar\\\\character01\\\\rot180',\n",
       " 'Tengwar\\\\character01\\\\rot270',\n",
       " 'Tengwar\\\\character02\\\\rot000',\n",
       " 'Tengwar\\\\character02\\\\rot090',\n",
       " 'Tengwar\\\\character02\\\\rot180',\n",
       " 'Tengwar\\\\character02\\\\rot270',\n",
       " 'Tengwar\\\\character03\\\\rot000',\n",
       " 'Tengwar\\\\character03\\\\rot090',\n",
       " 'Tengwar\\\\character03\\\\rot180',\n",
       " 'Tengwar\\\\character03\\\\rot270',\n",
       " 'Tengwar\\\\character04\\\\rot000',\n",
       " 'Tengwar\\\\character04\\\\rot090',\n",
       " 'Tengwar\\\\character04\\\\rot180',\n",
       " 'Tengwar\\\\character04\\\\rot270',\n",
       " 'Tengwar\\\\character05\\\\rot000',\n",
       " 'Tengwar\\\\character05\\\\rot090',\n",
       " 'Tengwar\\\\character05\\\\rot180',\n",
       " 'Tengwar\\\\character05\\\\rot270',\n",
       " 'Tengwar\\\\character06\\\\rot000',\n",
       " 'Tengwar\\\\character06\\\\rot090',\n",
       " 'Tengwar\\\\character06\\\\rot180',\n",
       " 'Tengwar\\\\character06\\\\rot270',\n",
       " 'Tengwar\\\\character07\\\\rot000',\n",
       " 'Tengwar\\\\character07\\\\rot090',\n",
       " 'Tengwar\\\\character07\\\\rot180',\n",
       " 'Tengwar\\\\character07\\\\rot270',\n",
       " 'Tengwar\\\\character08\\\\rot000',\n",
       " 'Tengwar\\\\character08\\\\rot090',\n",
       " 'Tengwar\\\\character08\\\\rot180',\n",
       " 'Tengwar\\\\character08\\\\rot270',\n",
       " 'Tengwar\\\\character09\\\\rot000',\n",
       " 'Tengwar\\\\character09\\\\rot090',\n",
       " 'Tengwar\\\\character09\\\\rot180',\n",
       " 'Tengwar\\\\character09\\\\rot270',\n",
       " 'Tengwar\\\\character10\\\\rot000',\n",
       " 'Tengwar\\\\character10\\\\rot090',\n",
       " 'Tengwar\\\\character10\\\\rot180',\n",
       " 'Tengwar\\\\character10\\\\rot270',\n",
       " 'Tengwar\\\\character11\\\\rot000',\n",
       " 'Tengwar\\\\character11\\\\rot090',\n",
       " 'Tengwar\\\\character11\\\\rot180',\n",
       " 'Tengwar\\\\character11\\\\rot270',\n",
       " 'Tengwar\\\\character12\\\\rot000',\n",
       " 'Tengwar\\\\character12\\\\rot090',\n",
       " 'Tengwar\\\\character12\\\\rot180',\n",
       " 'Tengwar\\\\character12\\\\rot270',\n",
       " 'Tengwar\\\\character13\\\\rot000',\n",
       " 'Tengwar\\\\character13\\\\rot090',\n",
       " 'Tengwar\\\\character13\\\\rot180',\n",
       " 'Tengwar\\\\character13\\\\rot270',\n",
       " 'Tengwar\\\\character14\\\\rot000',\n",
       " 'Tengwar\\\\character14\\\\rot090',\n",
       " 'Tengwar\\\\character14\\\\rot180',\n",
       " 'Tengwar\\\\character14\\\\rot270',\n",
       " 'Tengwar\\\\character15\\\\rot000',\n",
       " 'Tengwar\\\\character15\\\\rot090',\n",
       " 'Tengwar\\\\character15\\\\rot180',\n",
       " 'Tengwar\\\\character15\\\\rot270',\n",
       " 'Tengwar\\\\character16\\\\rot000',\n",
       " 'Tengwar\\\\character16\\\\rot090',\n",
       " 'Tengwar\\\\character16\\\\rot180',\n",
       " 'Tengwar\\\\character16\\\\rot270',\n",
       " 'Tengwar\\\\character17\\\\rot000',\n",
       " 'Tengwar\\\\character17\\\\rot090',\n",
       " 'Tengwar\\\\character17\\\\rot180',\n",
       " 'Tengwar\\\\character17\\\\rot270',\n",
       " 'Tengwar\\\\character18\\\\rot000',\n",
       " 'Tengwar\\\\character18\\\\rot090',\n",
       " 'Tengwar\\\\character18\\\\rot180',\n",
       " 'Tengwar\\\\character18\\\\rot270',\n",
       " 'Tengwar\\\\character19\\\\rot000',\n",
       " 'Tengwar\\\\character19\\\\rot090',\n",
       " 'Tengwar\\\\character19\\\\rot180',\n",
       " 'Tengwar\\\\character19\\\\rot270',\n",
       " 'Tengwar\\\\character20\\\\rot000',\n",
       " 'Tengwar\\\\character20\\\\rot090',\n",
       " 'Tengwar\\\\character20\\\\rot180',\n",
       " 'Tengwar\\\\character20\\\\rot270',\n",
       " 'Tengwar\\\\character21\\\\rot000',\n",
       " 'Tengwar\\\\character21\\\\rot090',\n",
       " 'Tengwar\\\\character21\\\\rot180',\n",
       " 'Tengwar\\\\character21\\\\rot270',\n",
       " 'Tengwar\\\\character22\\\\rot000',\n",
       " 'Tengwar\\\\character22\\\\rot090',\n",
       " 'Tengwar\\\\character22\\\\rot180',\n",
       " 'Tengwar\\\\character22\\\\rot270',\n",
       " 'Tengwar\\\\character23\\\\rot000',\n",
       " 'Tengwar\\\\character23\\\\rot090',\n",
       " 'Tengwar\\\\character23\\\\rot180',\n",
       " 'Tengwar\\\\character23\\\\rot270',\n",
       " 'Tengwar\\\\character24\\\\rot000',\n",
       " 'Tengwar\\\\character24\\\\rot090',\n",
       " 'Tengwar\\\\character24\\\\rot180',\n",
       " 'Tengwar\\\\character24\\\\rot270',\n",
       " 'Tengwar\\\\character25\\\\rot000',\n",
       " 'Tengwar\\\\character25\\\\rot090',\n",
       " 'Tengwar\\\\character25\\\\rot180',\n",
       " 'Tengwar\\\\character25\\\\rot270',\n",
       " 'Tibetan\\\\character01\\\\rot000',\n",
       " 'Tibetan\\\\character01\\\\rot090',\n",
       " 'Tibetan\\\\character01\\\\rot180',\n",
       " 'Tibetan\\\\character01\\\\rot270',\n",
       " 'Tibetan\\\\character02\\\\rot000',\n",
       " 'Tibetan\\\\character02\\\\rot090',\n",
       " 'Tibetan\\\\character02\\\\rot180',\n",
       " 'Tibetan\\\\character02\\\\rot270',\n",
       " 'Tibetan\\\\character03\\\\rot000',\n",
       " 'Tibetan\\\\character03\\\\rot090',\n",
       " 'Tibetan\\\\character03\\\\rot180',\n",
       " 'Tibetan\\\\character03\\\\rot270',\n",
       " 'Tibetan\\\\character04\\\\rot000',\n",
       " 'Tibetan\\\\character04\\\\rot090',\n",
       " 'Tibetan\\\\character04\\\\rot180',\n",
       " 'Tibetan\\\\character04\\\\rot270',\n",
       " 'Tibetan\\\\character05\\\\rot000',\n",
       " 'Tibetan\\\\character05\\\\rot090',\n",
       " 'Tibetan\\\\character05\\\\rot180',\n",
       " 'Tibetan\\\\character05\\\\rot270',\n",
       " 'Tibetan\\\\character06\\\\rot000',\n",
       " 'Tibetan\\\\character06\\\\rot090',\n",
       " 'Tibetan\\\\character06\\\\rot180',\n",
       " 'Tibetan\\\\character06\\\\rot270',\n",
       " 'Tibetan\\\\character07\\\\rot000',\n",
       " 'Tibetan\\\\character07\\\\rot090',\n",
       " 'Tibetan\\\\character07\\\\rot180',\n",
       " 'Tibetan\\\\character07\\\\rot270',\n",
       " 'Tibetan\\\\character08\\\\rot000',\n",
       " 'Tibetan\\\\character08\\\\rot090',\n",
       " 'Tibetan\\\\character08\\\\rot180',\n",
       " 'Tibetan\\\\character08\\\\rot270',\n",
       " 'Tibetan\\\\character09\\\\rot000',\n",
       " 'Tibetan\\\\character09\\\\rot090',\n",
       " 'Tibetan\\\\character09\\\\rot180',\n",
       " 'Tibetan\\\\character09\\\\rot270',\n",
       " 'Tibetan\\\\character10\\\\rot000',\n",
       " 'Tibetan\\\\character10\\\\rot090',\n",
       " 'Tibetan\\\\character10\\\\rot180',\n",
       " 'Tibetan\\\\character10\\\\rot270',\n",
       " 'Tibetan\\\\character11\\\\rot000',\n",
       " 'Tibetan\\\\character11\\\\rot090',\n",
       " 'Tibetan\\\\character11\\\\rot180',\n",
       " 'Tibetan\\\\character11\\\\rot270',\n",
       " 'Tibetan\\\\character12\\\\rot000',\n",
       " 'Tibetan\\\\character12\\\\rot090',\n",
       " 'Tibetan\\\\character12\\\\rot180',\n",
       " 'Tibetan\\\\character12\\\\rot270',\n",
       " 'Tibetan\\\\character13\\\\rot000',\n",
       " 'Tibetan\\\\character13\\\\rot090',\n",
       " 'Tibetan\\\\character13\\\\rot180',\n",
       " 'Tibetan\\\\character13\\\\rot270',\n",
       " 'Tibetan\\\\character14\\\\rot000',\n",
       " 'Tibetan\\\\character14\\\\rot090',\n",
       " 'Tibetan\\\\character14\\\\rot180',\n",
       " 'Tibetan\\\\character14\\\\rot270',\n",
       " 'Tibetan\\\\character15\\\\rot000',\n",
       " 'Tibetan\\\\character15\\\\rot090',\n",
       " 'Tibetan\\\\character15\\\\rot180',\n",
       " 'Tibetan\\\\character15\\\\rot270',\n",
       " 'Tibetan\\\\character16\\\\rot000',\n",
       " 'Tibetan\\\\character16\\\\rot090',\n",
       " 'Tibetan\\\\character16\\\\rot180',\n",
       " 'Tibetan\\\\character16\\\\rot270',\n",
       " 'Tibetan\\\\character17\\\\rot000',\n",
       " 'Tibetan\\\\character17\\\\rot090',\n",
       " 'Tibetan\\\\character17\\\\rot180',\n",
       " 'Tibetan\\\\character17\\\\rot270',\n",
       " 'Tibetan\\\\character18\\\\rot000',\n",
       " 'Tibetan\\\\character18\\\\rot090',\n",
       " 'Tibetan\\\\character18\\\\rot180',\n",
       " 'Tibetan\\\\character18\\\\rot270',\n",
       " 'Tibetan\\\\character19\\\\rot000',\n",
       " 'Tibetan\\\\character19\\\\rot090',\n",
       " 'Tibetan\\\\character19\\\\rot180',\n",
       " 'Tibetan\\\\character19\\\\rot270',\n",
       " 'Tibetan\\\\character20\\\\rot000',\n",
       " 'Tibetan\\\\character20\\\\rot090',\n",
       " 'Tibetan\\\\character20\\\\rot180',\n",
       " 'Tibetan\\\\character20\\\\rot270',\n",
       " 'Tibetan\\\\character21\\\\rot000',\n",
       " 'Tibetan\\\\character21\\\\rot090',\n",
       " 'Tibetan\\\\character21\\\\rot180',\n",
       " 'Tibetan\\\\character21\\\\rot270',\n",
       " 'Tibetan\\\\character22\\\\rot000',\n",
       " 'Tibetan\\\\character22\\\\rot090',\n",
       " 'Tibetan\\\\character22\\\\rot180',\n",
       " 'Tibetan\\\\character22\\\\rot270',\n",
       " 'Tibetan\\\\character23\\\\rot000',\n",
       " 'Tibetan\\\\character23\\\\rot090',\n",
       " 'Tibetan\\\\character23\\\\rot180',\n",
       " 'Tibetan\\\\character23\\\\rot270',\n",
       " 'Tibetan\\\\character24\\\\rot000',\n",
       " 'Tibetan\\\\character24\\\\rot090',\n",
       " 'Tibetan\\\\character24\\\\rot180',\n",
       " 'Tibetan\\\\character24\\\\rot270',\n",
       " 'Tibetan\\\\character25\\\\rot000',\n",
       " 'Tibetan\\\\character25\\\\rot090',\n",
       " 'Tibetan\\\\character25\\\\rot180',\n",
       " 'Tibetan\\\\character25\\\\rot270',\n",
       " 'Tibetan\\\\character26\\\\rot000',\n",
       " 'Tibetan\\\\character26\\\\rot090',\n",
       " 'Tibetan\\\\character26\\\\rot180',\n",
       " 'Tibetan\\\\character26\\\\rot270',\n",
       " 'Tibetan\\\\character27\\\\rot000',\n",
       " 'Tibetan\\\\character27\\\\rot090',\n",
       " 'Tibetan\\\\character27\\\\rot180',\n",
       " 'Tibetan\\\\character27\\\\rot270',\n",
       " 'Tibetan\\\\character28\\\\rot000',\n",
       " 'Tibetan\\\\character28\\\\rot090',\n",
       " 'Tibetan\\\\character28\\\\rot180',\n",
       " 'Tibetan\\\\character28\\\\rot270',\n",
       " 'Tibetan\\\\character29\\\\rot000',\n",
       " 'Tibetan\\\\character29\\\\rot090',\n",
       " 'Tibetan\\\\character29\\\\rot180',\n",
       " 'Tibetan\\\\character29\\\\rot270',\n",
       " 'Tibetan\\\\character30\\\\rot000',\n",
       " 'Tibetan\\\\character30\\\\rot090',\n",
       " 'Tibetan\\\\character30\\\\rot180',\n",
       " 'Tibetan\\\\character30\\\\rot270',\n",
       " 'Tibetan\\\\character31\\\\rot000',\n",
       " 'Tibetan\\\\character31\\\\rot090',\n",
       " 'Tibetan\\\\character31\\\\rot180',\n",
       " 'Tibetan\\\\character31\\\\rot270',\n",
       " 'Tibetan\\\\character32\\\\rot000',\n",
       " 'Tibetan\\\\character32\\\\rot090',\n",
       " 'Tibetan\\\\character32\\\\rot180',\n",
       " 'Tibetan\\\\character32\\\\rot270',\n",
       " 'Tibetan\\\\character33\\\\rot000',\n",
       " 'Tibetan\\\\character33\\\\rot090',\n",
       " 'Tibetan\\\\character33\\\\rot180',\n",
       " 'Tibetan\\\\character33\\\\rot270',\n",
       " 'Tibetan\\\\character34\\\\rot000',\n",
       " 'Tibetan\\\\character34\\\\rot090',\n",
       " 'Tibetan\\\\character34\\\\rot180',\n",
       " 'Tibetan\\\\character34\\\\rot270',\n",
       " 'Tibetan\\\\character35\\\\rot000',\n",
       " 'Tibetan\\\\character35\\\\rot090',\n",
       " 'Tibetan\\\\character35\\\\rot180',\n",
       " 'Tibetan\\\\character35\\\\rot270',\n",
       " 'Tibetan\\\\character36\\\\rot000',\n",
       " 'Tibetan\\\\character36\\\\rot090',\n",
       " 'Tibetan\\\\character36\\\\rot180',\n",
       " 'Tibetan\\\\character36\\\\rot270',\n",
       " 'Tibetan\\\\character37\\\\rot000',\n",
       " 'Tibetan\\\\character37\\\\rot090',\n",
       " 'Tibetan\\\\character37\\\\rot180',\n",
       " 'Tibetan\\\\character37\\\\rot270',\n",
       " 'Tibetan\\\\character38\\\\rot000',\n",
       " 'Tibetan\\\\character38\\\\rot090',\n",
       " 'Tibetan\\\\character38\\\\rot180',\n",
       " 'Tibetan\\\\character38\\\\rot270',\n",
       " 'Tibetan\\\\character39\\\\rot000',\n",
       " 'Tibetan\\\\character39\\\\rot090',\n",
       " 'Tibetan\\\\character39\\\\rot180',\n",
       " 'Tibetan\\\\character39\\\\rot270',\n",
       " 'Tibetan\\\\character40\\\\rot000',\n",
       " 'Tibetan\\\\character40\\\\rot090',\n",
       " 'Tibetan\\\\character40\\\\rot180',\n",
       " 'Tibetan\\\\character40\\\\rot270',\n",
       " 'Tibetan\\\\character41\\\\rot000',\n",
       " 'Tibetan\\\\character41\\\\rot090',\n",
       " 'Tibetan\\\\character41\\\\rot180',\n",
       " 'Tibetan\\\\character41\\\\rot270',\n",
       " 'Tibetan\\\\character42\\\\rot000',\n",
       " 'Tibetan\\\\character42\\\\rot090',\n",
       " 'Tibetan\\\\character42\\\\rot180',\n",
       " 'Tibetan\\\\character42\\\\rot270']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda s: s.startswith('T'), test_data.idx_classes.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441e1a9",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "- at each step we need to sample an episode.\n",
    "- an episode corresponds to the task of classifying a subset of classes\n",
    "- each episode provides a small (few-shot) query and support set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2da90df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler(object):\n",
    "    '''\n",
    "    PrototypicalBatchSampler: yield a batch of indexes at each iteration.\n",
    "    Indexes are calculated by keeping in account 'classes_per_it' and 'num_samples',\n",
    "    In fact at every iteration the batch indexes will refer to  'num_support' + 'num_query' samples\n",
    "    for 'classes_per_it' random classes.\n",
    "\n",
    "    __len__ returns the number of episodes per epoch (same as 'self.iterations').\n",
    "    '''\n",
    "\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        '''\n",
    "        Initialize the PrototypicalBatchSampler object\n",
    "        Args:\n",
    "        - labels: an iterable containing all the labels for the current dataset\n",
    "        samples indexes will be infered from this iterable.\n",
    "        - classes_per_it: number of random classes for each iteration\n",
    "        - num_samples: number of samples for each iteration for each class (support + query)\n",
    "        - iterations: number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        super(PrototypicalBatchSampler, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        # create a matrix, indexes, of dim: classes X max(elements per class)\n",
    "        # fill it with nans\n",
    "        # for every class c, fill the relative row with the indices samples belonging to c\n",
    "        # in numel_per_class we store the number of samples for each class/row\n",
    "        self.idxs = range(len(self.labels))\n",
    "        self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
    "        self.indexes = torch.Tensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros_like(self.classes)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            label_idx = np.argwhere(self.classes == label).item()\n",
    "            self.indexes[label_idx, np.where(np.isnan(self.indexes[label_idx]))[0][0]] = idx\n",
    "            self.numel_per_class[label_idx] += 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        yield a batch of indexes\n",
    "        '''\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for it in range(self.iterations):\n",
    "            batch_size = spc * cpi\n",
    "            batch = torch.LongTensor(batch_size)\n",
    "            \n",
    "            # NOTE: pick classes randomly\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]\n",
    "\n",
    "            # NOTE: for each class, pick a limited set of samples randomly\n",
    "            for i, c in enumerate(self.classes[c_idxs]):\n",
    "                s = slice(i * spc, (i + 1) * spc)\n",
    "                # FIXME when torch.argwhere will exists\n",
    "                label_idx = torch.arange(len(self.classes)).long()[self.classes == c].item()\n",
    "                sample_idxs = torch.randperm(self.numel_per_class[label_idx])[:spc]\n",
    "                batch[s] = self.indexes[label_idx][sample_idxs]\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        returns the number of iterations (episodes) per epoch\n",
    "        '''\n",
    "        return self.iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645da433",
   "metadata": {},
   "source": [
    "NOTE: classes_per_it combines the query and support set. We split them inside the prototypical loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad8dbbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_per_it = options.classes_per_it_tr\n",
    "num_samples = options.num_support_tr + options.num_query_tr\n",
    "\n",
    "sampler = PrototypicalBatchSampler(\n",
    "    labels=train_data.y,\n",
    "    classes_per_it=7,\n",
    "    num_samples=10,\n",
    "    iterations=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a07dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1271, 2595,  913, 1757, 1083,  913, 2595,  157, 1271, 1083, 1271, 3125,\n",
      "        2595, 1083,  157, 2595,  913, 1083, 2595, 1083, 1757, 1271, 2595,  157,\n",
      "        1271, 1083, 1757, 2595, 1083,  157, 1757, 1757, 1757, 3125, 1271,  157,\n",
      "        1271,  913, 3125, 2595,  913,  913, 3125, 3125, 3125, 1083, 3125,  913,\n",
      "         157,  913, 1083,  157, 1757, 1271,  913, 2595,  157, 3125, 1757, 1271,\n",
      "         157,  913,  157, 1757, 1757, 1083, 1271, 2595, 3125, 3125])\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 29, 30, 31, 33, 35, 37, 38, 40, 42, 43]\n",
      "[22, 25, 27, 28, 32, 34, 36, 39, 41, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_data, batch_sampler=sampler)\n",
    "\n",
    "for episode in dataloader:\n",
    "    x, y = episode \n",
    "    print(y)\n",
    "    \n",
    "    classes = torch.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # 10 samples in total, 5 for query, 5 for support\n",
    "    n_support = 5\n",
    "    n_query = (y == classes[0].item()).sum().item() - n_support\n",
    "    \n",
    "    # NOTE: select support samples   \n",
    "    support_idxs = torch.stack(list(map(lambda c: (y == c).nonzero()[:n_support], classes))).view(-1)\n",
    "    support_samples = x[support_idxs]\n",
    "    print(sorted(support_idxs.tolist()))\n",
    "    \n",
    "    # NOTE: select query samples\n",
    "    query_idxs = torch.stack(list(map(lambda c: (y == c).nonzero()[n_support:], classes))).view(-1)\n",
    "    query_samples = x[query_idxs]\n",
    "    print(sorted(query_idxs.tolist()))\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5cdde7",
   "metadata": {},
   "source": [
    "# Model\n",
    "The embedding network is a basic CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe0726e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://github.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    '''\n",
    "    returns a block conv-bn-relu-pool\n",
    "    '''\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    '''\n",
    "    Model as described in the reference paper,\n",
    "    source: https://github.com/jakesnell/prototypical-networks/blob/f0c48808e496989d01db59f86d4449d7aee9ab0c/protonets/models/few_shot.py#L62-L84\n",
    "    '''\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(x_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52314d2c",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "takes the episode data as input:\n",
    "- split query and support sets\n",
    "- compute the prototypes by averaging the features of n_support samples for each class in target\n",
    "- compute the distances from each samples' features to each prototype\n",
    "- compute the class probabilities for each sample using the distance wrt prototypes\n",
    "\n",
    "first, we define the pairwise euclidean distance between two sets of vectors (query and support set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87f463d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efa48eb",
   "metadata": {},
   "source": [
    "Next, we define the prototypical loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e797240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prototypical_loss(input, target, n_support):\n",
    "    '''\n",
    "    Inspired by https://github.com/jakesnell/prototypical-networks/blob/master/protonets/models/few_shot.py\n",
    "\n",
    "    Compute the barycentres by averaging the features of n_support\n",
    "    samples for each class in target, computes then the distances from each\n",
    "    samples' features to each one of the barycentres, computes the\n",
    "    log_probability for each n_query samples for each one of the current\n",
    "    classes, of appartaining to a class c, loss and accuracy are then computed\n",
    "    and returned\n",
    "    \n",
    "    Args:\n",
    "    - input: the model output for a batch of samples\n",
    "    - target: ground truth for the above batch of samples\n",
    "    - n_support: number of samples to keep in account when computing\n",
    "      barycentres, for each one of the current classes\n",
    "    '''\n",
    "    target_cpu = target.to('cpu')\n",
    "    input_cpu = input.to('cpu')\n",
    "\n",
    "    def supp_idxs(c):\n",
    "        # FIXME when torch will support where as np\n",
    "        return target_cpu.eq(c).nonzero()[:n_support].squeeze(1)\n",
    "\n",
    "    # FIXME when torch.unique will be available on cuda too\n",
    "    classes = torch.unique(target_cpu)\n",
    "    n_classes = len(classes)\n",
    "    # FIXME when torch will support where as np\n",
    "    # assuming n_query, n_target constants\n",
    "    n_query = target_cpu.eq(classes[0].item()).sum().item() - n_support\n",
    "    \n",
    "    # NOTE: select support samples and split them by classes\n",
    "    support_idxs = list(map(supp_idxs, classes))\n",
    "\n",
    "    # NOTE: compute prototypes by averaging embeddings for each class\n",
    "    prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in support_idxs])\n",
    "    \n",
    "    # NOTE: select query samples\n",
    "    # FIXME when torch will support where as np\n",
    "    query_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[n_support:], classes))).view(-1)\n",
    "    query_samples = input.to('cpu')[query_idxs]\n",
    "\n",
    "    # NOTE: compute distances between query samples and prototypes\n",
    "    dists = euclidean_dist(query_samples, prototypes)\n",
    "    \n",
    "    # NOTE: compute probabilities (actually, logits) from the distances\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)\n",
    "    \n",
    "    # NOTE: once you have the logits, you can compute the crossentropy loss and accuracy as usual \n",
    "    target_inds = torch.arange(0, n_classes)\n",
    "    target_inds = target_inds.view(n_classes, 1, 1)\n",
    "    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n",
    "\n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)\n",
    "    acc_val = y_hat.eq(target_inds.squeeze(2)).float().mean()\n",
    "\n",
    "    return loss_val,  acc_val\n",
    "\n",
    "\n",
    "class PrototypicalLoss(Module):\n",
    "    \"\"\"wrapper for prototypical_loss\"\"\"\n",
    "    def __init__(self, n_support):\n",
    "        super().__init__()\n",
    "        self.n_support = n_support\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return prototypical_loss(input, target, self.n_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24025c3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d207520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_sampler(opt, labels, mode):\n",
    "    if 'train' in mode:\n",
    "        classes_per_it = opt.classes_per_it_tr\n",
    "        num_samples = opt.num_support_tr + opt.num_query_tr\n",
    "    else:\n",
    "        classes_per_it = opt.classes_per_it_val\n",
    "        num_samples = opt.num_support_val + opt.num_query_val\n",
    "    \n",
    "    # NOTE: classes_per_it combines the query and support set\n",
    "    # we split them inside the prototypical loss\n",
    "    return PrototypicalBatchSampler(labels=labels,\n",
    "                                    classes_per_it=classes_per_it,\n",
    "                                    num_samples=num_samples,\n",
    "                                    iterations=opt.iterations)\n",
    "\n",
    "\n",
    "def init_dataloader(opt, mode):\n",
    "    dataset = load_omniglot_data(opt, mode)\n",
    "    sampler = init_sampler(opt, dataset.y, mode)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def init_protonet(opt):\n",
    "    '''\n",
    "    Initialize the ProtoNet\n",
    "    '''\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "    model = ProtoNet().to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def init_optim(opt, model):\n",
    "    '''\n",
    "    Initialize optimizer\n",
    "    '''\n",
    "    return torch.optim.Adam(params=model.parameters(),\n",
    "                            lr=opt.learning_rate)\n",
    "\n",
    "\n",
    "def init_lr_scheduler(opt, optim):\n",
    "    '''\n",
    "    Initialize the learning rate scheduler\n",
    "    '''\n",
    "    return torch.optim.lr_scheduler.StepLR(optimizer=optim,\n",
    "                                           gamma=opt.lr_scheduler_gamma,\n",
    "                                           step_size=opt.lr_scheduler_step)\n",
    "\n",
    "\n",
    "def save_list_to_file(path, thelist):\n",
    "    with open(path, 'w') as f:\n",
    "        for item in thelist:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "\n",
    "def train(opt, tr_dataloader, model, optim, lr_scheduler, val_dataloader=None):\n",
    "    '''\n",
    "    Train the model with the prototypical learning algorithm\n",
    "    '''\n",
    "\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "\n",
    "    if val_dataloader is None:\n",
    "        best_state = None\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    best_acc = 0\n",
    "\n",
    "    best_model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
    "    last_model_path = os.path.join(opt.experiment_root, 'last_model.pth')\n",
    "\n",
    "    for epoch in range(opt.epochs):\n",
    "        print('=== Epoch: {} ==='.format(epoch))\n",
    "        tr_iter = iter(tr_dataloader)\n",
    "        model.train()\n",
    "        for batch in tqdm(tr_iter):\n",
    "            optim.zero_grad()\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model_output = model(x)\n",
    "            loss, acc = prototypical_loss(model_output, target=y,\n",
    "                                n_support=opt.num_support_tr)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "            train_acc.append(acc.item())\n",
    "        avg_loss = np.mean(train_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(train_acc[-opt.iterations:])\n",
    "        print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
    "        lr_scheduler.step()\n",
    "        if val_dataloader is None:\n",
    "            continue\n",
    "        val_iter = iter(val_dataloader)\n",
    "        model.eval()\n",
    "        for batch in val_iter:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model_output = model(x)\n",
    "            loss, acc = prototypical_loss(model_output, target=y,\n",
    "                                n_support=opt.num_support_val)\n",
    "            val_loss.append(loss.item())\n",
    "            val_acc.append(acc.item())\n",
    "        avg_loss = np.mean(val_loss[-opt.iterations:])\n",
    "        avg_acc = np.mean(val_acc[-opt.iterations:])\n",
    "        postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(\n",
    "            best_acc)\n",
    "        print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(\n",
    "            avg_loss, avg_acc, postfix))\n",
    "        if avg_acc >= best_acc:\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            best_acc = avg_acc\n",
    "            best_state = model.state_dict()\n",
    "\n",
    "    torch.save(model.state_dict(), last_model_path)\n",
    "\n",
    "    for name in ['train_loss', 'train_acc', 'val_loss', 'val_acc']:\n",
    "        save_list_to_file(os.path.join(opt.experiment_root,\n",
    "                                       name + '.txt'), locals()[name])\n",
    "\n",
    "    return best_state, best_acc, train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "def test(opt, test_dataloader, model):\n",
    "    '''\n",
    "    Test the model trained with the prototypical learning algorithm\n",
    "    '''\n",
    "    device = 'cuda:0' if torch.cuda.is_available() and opt.cuda else 'cpu'\n",
    "    avg_acc = list()\n",
    "    for epoch in range(10):\n",
    "        test_iter = iter(test_dataloader)\n",
    "        for batch in test_iter:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            model_output = model(x)\n",
    "            _, acc = prototypical_loss(model_output, target=y,\n",
    "                             n_support=opt.num_support_val)\n",
    "            avg_acc.append(acc.item())\n",
    "    avg_acc = np.mean(avg_acc)\n",
    "    print('Test Acc: {}'.format(avg_acc))\n",
    "    return avg_acc\n",
    "\n",
    "\n",
    "def eval(opt):\n",
    "    '''\n",
    "    Initialize everything and train\n",
    "    '''\n",
    "    options = get_parser().parse_args()\n",
    "\n",
    "    if torch.cuda.is_available() and not options.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "    init_seed(options)\n",
    "    test_dataloader = load_omniglot_data(options)[-1]\n",
    "    model = init_protonet(options)\n",
    "    model_path = os.path.join(opt.experiment_root, 'best_model.pth')\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    test(opt=options,\n",
    "         test_dataloader=test_dataloader,\n",
    "         model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45414f3b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b11c8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Dataset: Found 82240 items \n",
      "== Dataset: Found 4112 classes\n",
      "== Dataset: Found 13760 items \n",
      "== Dataset: Found 688 classes\n",
      "== Dataset: Found 33840 items \n",
      "== Dataset: Found 1692 classes\n",
      "=== Epoch: 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:10<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 1.7953407049179078, Avg Train Acc: 0.5656666725873947\n",
      "Avg Val Loss: 1.5017887473106384, Avg Val Acc: 0.8173333406448364 (Best)\n",
      "Testing with last model..\n",
      "Test Acc: 0.8610666674375534\n",
      "Testing with best model..\n",
      "Test Acc: 0.8561333328485489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8561333328485489"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(options.experiment_root, exist_ok=True)\n",
    "init_seed(options)\n",
    "\n",
    "tr_dataloader = init_dataloader(options, 'train')\n",
    "val_dataloader = init_dataloader(options, 'val')\n",
    "test_dataloader = init_dataloader(options, 'test')\n",
    "\n",
    "model = init_protonet(options)\n",
    "optim = init_optim(options, model)\n",
    "lr_scheduler = init_lr_scheduler(options, optim)\n",
    "\n",
    "# meta-train\n",
    "res = train(opt=options,\n",
    "            tr_dataloader=tr_dataloader,\n",
    "            val_dataloader=val_dataloader,\n",
    "            model=model,\n",
    "            optim=optim,\n",
    "            lr_scheduler=lr_scheduler)\n",
    "best_state, best_acc, train_loss, train_acc, val_loss, val_acc = res\n",
    "\n",
    "# meta-test\n",
    "print('Testing with last model..')\n",
    "test(opt=options,\n",
    "     test_dataloader=test_dataloader,\n",
    "     model=model)\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print('Testing with best model..')\n",
    "test(opt=options,\n",
    "     test_dataloader=test_dataloader,\n",
    "     model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf82054",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "- **plot embeddings**: you can use a dimensionality reduction method (PCA, t-SNE) to reduce the data into 2 dimensions and plot the prototypes together with the query set. Are the prototypes close to the real examples? Are they far from each other?\n",
    "- **siamese networks**: try to implement siamese networks. You can reuse the omniglot dataloader, training loop and anything else you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
