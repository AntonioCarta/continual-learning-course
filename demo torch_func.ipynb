{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from https://pytorch.org/docs/stable/func.whirlwind_tour.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.func import vjp, jvp, grad, vmap, hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful vs Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "\n",
    "def f(W):\n",
    "    return ((W ** 2)@x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateful API:  [[0.16340939700603485, 0.005695031955838203]]\n"
     ]
    }
   ],
   "source": [
    "W.grad = None  # reset gradient (optimizer.zero_grad)\n",
    "l = f(W)\n",
    "l.backward()\n",
    "print(\"stateful API: \", W.grad.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional API:  [[0.16340939700603485, 0.005695031955838203]]\n"
     ]
    }
   ],
   "source": [
    "gw = grad(f)(W)\n",
    "print(\"functional API: \", gw.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop over gradient operations\n",
    "\n",
    "we are going to see some toy examples of gradient computations.\n",
    "Here are the equations of the function and its derivatives so that you can check the results are correct:\n",
    "\n",
    "\\begin{aligned}\n",
    "& y=w^2 \\cdot x=\\left[\\begin{array}{l}\n",
    "w_1^2 x_1 \\\\\n",
    "w_1^2 x_2\n",
    "\\end{array}\\right] \\\\\n",
    "& \\frac{\\partial y}{\\partial w}=2 w x=\\left[\\begin{array}{c}\n",
    "2 w_1 x \\\\\n",
    "2 w_2 x\n",
    "\\end{array}\\right] \\\\\n",
    "& \\frac{\\partial y}{\\partial w_i \\partial w_j}=2 x=\\left[\\begin{array}{cc}\n",
    "2 x_1 & 0 \\\\\n",
    "0 & 2 x_2\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfoo/dW:\t [[3.80531907081604, -2.361640691757202]]\n",
      "expect: \t [[3.80531907081604, -2.361640691757202]]\n",
      "\n",
      "Vector-Jacobian Product\n",
      "ones @ ddfoo/dW:\t [[3.0078017711639404, -2.051379919052124]]\n",
      "expect:          \t [3.0078017711639404, -2.051379919052124]\n",
      "\n",
      "Jacobian-Vector Product\n",
      "ddfoo/dW @ ones:\t [3.0078017711639404, -2.051379919052124]\n",
      "expect:          \t [3.0078017711639404, -2.051379919052124]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.func import vjp, jvp, grad\n",
    "\n",
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "ones = torch.tensor([1.0, 1.0]).reshape(1, 2)\n",
    "\n",
    "foo = lambda W: ((W ** 2)@x).sum()\n",
    "gw = grad(foo)(W)\n",
    "print(\"dfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", (2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "print(\"Vector-Jacobian Product\")\n",
    "_, jc = vjp(grad(foo), W)\n",
    "print(\"ones @ ddfoo/dW:\\t\", jc(ones)[0].tolist())\n",
    "print(\"expect:          \\t\", (2*x).tolist())\n",
    "print()\n",
    "\n",
    "print(\"Jacobian-Vector Product\")\n",
    "_, jc = jvp(grad(foo), (W,), (ones,))\n",
    "print(\"ddfoo/dW @ ones:\\t\", jc[0].tolist())\n",
    "print(\"expect:          \\t\", (2*x).tolist())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop over SGD step (full gradient and truncated)\n",
    "\n",
    "We are going to need the gradient over an SGD step to implement MAML.\n",
    "- We take the previous example and perform a gradient step\n",
    "- then, we compute the gradient of the gradient\n",
    "- there are two options:\n",
    "    - full gradient: the true gradient\n",
    "    - truncated gradient: the approximation where $\\nabla_{w} y$ is assumed constant when computing the gradient the second time\n",
    "\n",
    "SGD Step: $\\omega^*=\\omega-\\alpha \\nabla_w y=(1-a x) \\omega$\n",
    "\n",
    "model after SGD step: $y^*=\\left(w-\\alpha \\nabla_{w y}\\right)^2 x^*=(1-2 \\alpha x)^2 w^2 x^*$\n",
    "\n",
    "Full gradient:\n",
    "$\\nabla_\\omega y^*=(1-2 \\alpha x)^2 2 \\omega x^*$\n",
    "\n",
    "Truncated Gradient (a.k.a. first-order approximation), which consider the result of the gradient as a constant:\n",
    "$F O \\nabla_w y^*= 2 \\omega x^*(1- 2 \\alpha x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[1.0289456844329834, 0.9332138299942017]]\n",
      "x:  [-0.04671746864914894, 0.5176782011985779]\n",
      "\n",
      "dfoo/dW:\t [[-0.09613947570323944, 0.9662089347839355]]\n",
      "expect: \t [[-0.09613947570323944, 0.9662089347839355]]\n",
      "\n",
      "ddfoo/dW:\t [[-0.09794442355632782, 0.776492178440094]]\n",
      "expect: \t [[-0.09794443100690842, 0.776492178440094]]\n",
      "\n",
      "FO-ddfoo/dW:\t [[-0.09703775495290756, 0.8661718368530273]]\n",
      "expect: \t [[-0.09703775495290756, 0.8661718368530273]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.func import vjp, jvp\n",
    "\n",
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "ones = torch.tensor([1.0, 1.0]).reshape(1, 2)\n",
    "alpha = 0.1\n",
    "print(\"w: \", W.tolist())\n",
    "print(\"x: \", x.tolist())\n",
    "print()\n",
    "\n",
    "foo = lambda W: ((W ** 2)@x).sum()\n",
    "gw = grad(foo)(W)\n",
    "print(\"dfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", (2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "def foobar(W):\n",
    "    gw = grad(foo)(W)\n",
    "    wbar = W - alpha*gw\n",
    "    return foo(wbar)\n",
    "\n",
    "gw = grad(foobar)(W)\n",
    "print(\"ddfoo/dW:\\t\", gw.tolist())\n",
    "# print(\"dfoo/dW:\\t\", jc(ones)[0].tolist())\n",
    "print(\"expect: \\t\", (((1 -2*alpha*x) ** 2) * 2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "wbar = W - alpha*grad(foo)(W)\n",
    "gw = grad(foo)(wbar)\n",
    "print(\"FO-ddfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", ((1-alpha*2*x) * 2*W*x).tolist())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vmap\n",
    "vmap is a vectorization/batching transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 3.1131, 0.3854], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import vmap\n",
    "batch_size, feature_size = 3, 5\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "\n",
    "def model(feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    # note how this model computes the output for a single\n",
    "    # sample, not the whole minibatch\n",
    "    assert feature_vec.dim() == 1\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "result = vmap(model)(examples)  # auto-vectorization with vmap\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vmap + grad per-sample gradients (e.g. Fisher)\n",
    "Combining vmap and grad allows to compute per-sample gradients. Without vmap and grad, we would need to compute the gradient for each sample independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.6212,   1.9154,  -0.2961,  -0.7618,  -6.7760],\n",
       "        [  0.0000,   0.0000,  -0.0000,   0.0000,   0.0000],\n",
       "        [  7.0920,  -4.7760,   2.1690,  -6.6394, -14.7211]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.func import vmap\n",
    "batch_size, feature_size = 3, 5\n",
    "\n",
    "def model(weights,feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    assert feature_vec.dim() == 1\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "def compute_loss(weights, example, target):\n",
    "    y = model(weights, example)\n",
    "    return ((y - target) ** 2).mean()  # MSELoss\n",
    "\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "targets = torch.randn(batch_size)\n",
    "inputs = (weights,examples, targets)\n",
    "grad_weight_per_example = vmap(grad(compute_loss), in_dims=(None, 0, 0))(*inputs)\n",
    "\n",
    "glog = torch.log(grad_weight_per_example) \n",
    "glog.T * glog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "791aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
