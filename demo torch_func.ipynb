{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from https://pytorch.org/docs/stable/func.whirlwind_tour.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.func import vjp, jvp, grad, vmap, hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Computational Graph and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch operators build a computational graph under the hood\n",
    "- each operator defines a forward operation (the function application), and a backward (the gradient computation). During the forward, any information required by the backward is stored (e.g. activations).\n",
    "- The computational graph is used to compute the gradient\n",
    "- after the backward, every input tensors (the roots of the graph) contains its gradient w.r.t. the output\n",
    "- nn.Modules contain multiple tensors (their parameters) and use them to compute their outputs (forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141],\n",
      "        [ 1.4348,  0.4422, -1.0338, -2.0159,  1.4705, -0.3993,  0.4673,  1.9141]])\n"
     ]
    }
   ],
   "source": [
    "W = nn.Linear(8, 16)\n",
    "x = torch.randn(1, 8)\n",
    "\n",
    "assert W.weight.grad is None\n",
    "\n",
    "y = W(x).sum()\n",
    "y.backward()\n",
    "\n",
    "print(W.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can visualize the computational graph. We use torchviz, which traverses the computational graph and plots it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 10.0.1 (20240210.2158)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"498pt\" height=\"744pt\"\n",
       " viewBox=\"0.00 0.00 498.00 744.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 740)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-740 494,-740 494,4 -4,4\"/>\n",
       "<!-- 1230507152432 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>1230507152432</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"101,-32 47,-32 47,0 101,0 101,-32\"/>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-6.5\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 1230448920080 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1230448920080</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"148,-124 0,-124 0,-68 148,-68 148,-124\"/>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-110.5\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-98.5\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-86.5\" font-family=\"monospace\" font-size=\"10.00\">self_sym_numel: &#160;&#160;&#160;&#160;&#160;1</text>\n",
       "<text text-anchor=\"middle\" x=\"74\" y=\"-74.5\" font-family=\"monospace\" font-size=\"10.00\">self_sym_sizes: (1, 1)</text>\n",
       "</g>\n",
       "<!-- 1230448920080&#45;&gt;1230507152432 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>1230448920080&#45;&gt;1230507152432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M74,-67.72C74,-59.88 74,-51.34 74,-43.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.5,-43.62 74,-33.62 70.5,-43.62 77.5,-43.62\"/>\n",
       "</g>\n",
       "<!-- 1230448672720 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1230448672720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"302,-288 94,-288 94,-160 302,-160 302,-288\"/>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-274.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-262.5\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-250.5\" font-family=\"monospace\" font-size=\"10.00\">alpha &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-238.5\" font-family=\"monospace\" font-size=\"10.00\">beta &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-226.5\" font-family=\"monospace\" font-size=\"10.00\">mat1 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-214.5\" font-family=\"monospace\" font-size=\"10.00\">mat1_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 16)</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-202.5\" font-family=\"monospace\" font-size=\"10.00\">mat1_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;(16, 1)</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-190.5\" font-family=\"monospace\" font-size=\"10.00\">mat2 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-178.5\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;(16, 1)</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-166.5\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 16)</text>\n",
       "</g>\n",
       "<!-- 1230448672720&#45;&gt;1230448920080 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>1230448672720&#45;&gt;1230448920080</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M135.64,-159.63C126.5,-150.35 117.37,-141.07 109.05,-132.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"111.6,-130.21 102.09,-125.54 106.61,-135.12 111.6,-130.21\"/>\n",
       "</g>\n",
       "<!-- 1230507154448 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>1230507154448</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"230,-112 166,-112 166,-80 230,-80 230,-112\"/>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-98.5\" font-family=\"monospace\" font-size=\"10.00\">mat1</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-86.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 16)</text>\n",
       "</g>\n",
       "<!-- 1230448672720&#45;&gt;1230507154448 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1230448672720&#45;&gt;1230507154448</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198,-159.63C198,-141.98 198,-124.37 198,-112.34\"/>\n",
       "</g>\n",
       "<!-- 1230444483248 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>1230444483248</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"312,-112 248,-112 248,-80 312,-80 312,-112\"/>\n",
       "<text text-anchor=\"middle\" x=\"280\" y=\"-98.5\" font-family=\"monospace\" font-size=\"10.00\">mat2</text>\n",
       "<text text-anchor=\"middle\" x=\"280\" y=\"-86.5\" font-family=\"monospace\" font-size=\"10.00\"> (16, 1)</text>\n",
       "</g>\n",
       "<!-- 1230448672720&#45;&gt;1230444483248 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>1230448672720&#45;&gt;1230444483248</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.24,-159.63C250.73,-141.98 262.19,-124.37 270.02,-112.34\"/>\n",
       "</g>\n",
       "<!-- 1230448665520 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1230448665520</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"248,-356 148,-356 148,-336 248,-336 248,-356\"/>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-342.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1230448665520&#45;&gt;1230448672720 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>1230448665520&#45;&gt;1230448672720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198,-335.58C198,-327.17 198,-313.99 198,-299.62\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.5,-299.97 198,-289.97 194.5,-299.97 201.5,-299.97\"/>\n",
       "</g>\n",
       "<!-- 1230507154160 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>1230507154160</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"227,-484 169,-484 169,-452 227,-452 227,-484\"/>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-470.5\" font-family=\"monospace\" font-size=\"10.00\">W1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"198\" y=\"-458.5\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 1230507154160&#45;&gt;1230448665520 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1230507154160&#45;&gt;1230448665520</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M198,-451.8C198,-430.42 198,-391.4 198,-367.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"201.5,-367.74 198,-357.74 194.5,-367.74 201.5,-367.74\"/>\n",
       "</g>\n",
       "<!-- 1230448664416 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>1230448664416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"490,-368 342,-368 342,-324 490,-324 490,-368\"/>\n",
       "<text text-anchor=\"middle\" x=\"416\" y=\"-354.5\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "<text text-anchor=\"middle\" x=\"416\" y=\"-342.5\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"416\" y=\"-330.5\" font-family=\"monospace\" font-size=\"10.00\">result: [saved tensor]</text>\n",
       "</g>\n",
       "<!-- 1230448664416&#45;&gt;1230448672720 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>1230448664416&#45;&gt;1230448672720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M377.03,-323.55C358.59,-313.4 335.53,-300.7 312.15,-287.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"313.9,-284.8 303.45,-283.04 310.52,-290.93 313.9,-284.8\"/>\n",
       "</g>\n",
       "<!-- 1230444482288 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>1230444482288</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"448,-240 384,-240 384,-208 448,-208 448,-240\"/>\n",
       "<text text-anchor=\"middle\" x=\"416\" y=\"-226.5\" font-family=\"monospace\" font-size=\"10.00\">result</text>\n",
       "<text text-anchor=\"middle\" x=\"416\" y=\"-214.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 16)</text>\n",
       "</g>\n",
       "<!-- 1230448664416&#45;&gt;1230444482288 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>1230448664416&#45;&gt;1230444482288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M416,-323.55C416,-299.55 416,-261.28 416,-240.19\"/>\n",
       "</g>\n",
       "<!-- 1230448662736 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>1230448662736</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"486,-532 278,-532 278,-404 486,-404 486,-532\"/>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-518.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-506.5\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-494.5\" font-family=\"monospace\" font-size=\"10.00\">alpha &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-482.5\" font-family=\"monospace\" font-size=\"10.00\">beta &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-470.5\" font-family=\"monospace\" font-size=\"10.00\">mat1 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-458.5\" font-family=\"monospace\" font-size=\"10.00\">mat1_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 8)</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-446.5\" font-family=\"monospace\" font-size=\"10.00\">mat1_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;()</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-434.5\" font-family=\"monospace\" font-size=\"10.00\">mat2 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-422.5\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;(8, 16)</text>\n",
       "<text text-anchor=\"middle\" x=\"382\" y=\"-410.5\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 8)</text>\n",
       "</g>\n",
       "<!-- 1230448662736&#45;&gt;1230448664416 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>1230448662736&#45;&gt;1230448664416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M399.9,-403.84C402.3,-395.36 404.67,-386.97 406.84,-379.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"410.15,-380.48 409.51,-369.9 403.42,-378.57 410.15,-380.48\"/>\n",
       "</g>\n",
       "<!-- 1230448995856 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>1230448995856</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"324,-362 266,-362 266,-330 324,-330 324,-362\"/>\n",
       "<text text-anchor=\"middle\" x=\"295\" y=\"-348.5\" font-family=\"monospace\" font-size=\"10.00\">mat1</text>\n",
       "<text text-anchor=\"middle\" x=\"295\" y=\"-336.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 8)</text>\n",
       "</g>\n",
       "<!-- 1230448662736&#45;&gt;1230448995856 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>1230448662736&#45;&gt;1230448995856</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M336.21,-403.84C324.91,-388.25 313.82,-372.95 305.99,-362.16\"/>\n",
       "</g>\n",
       "<!-- 1230448672384 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>1230448672384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"485,-594 385,-594 385,-574 485,-574 485,-594\"/>\n",
       "<text text-anchor=\"middle\" x=\"435\" y=\"-580.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1230448672384&#45;&gt;1230448662736 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>1230448672384&#45;&gt;1230448662736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M430.66,-573.67C427.19,-566.2 421.96,-554.95 416.2,-542.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"419.52,-541.39 412.13,-533.8 413.17,-544.35 419.52,-541.39\"/>\n",
       "</g>\n",
       "<!-- 1230507152048 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>1230507152048</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"464,-668 406,-668 406,-636 464,-636 464,-668\"/>\n",
       "<text text-anchor=\"middle\" x=\"435\" y=\"-654.5\" font-family=\"monospace\" font-size=\"10.00\">W0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"435\" y=\"-642.5\" font-family=\"monospace\" font-size=\"10.00\"> (16)</text>\n",
       "</g>\n",
       "<!-- 1230507152048&#45;&gt;1230448672384 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>1230507152048&#45;&gt;1230448672384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M435,-635.69C435,-626.8 435,-615.46 435,-605.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"438.5,-605.83 435,-595.83 431.5,-605.83 438.5,-605.83\"/>\n",
       "</g>\n",
       "<!-- 1230448662784 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>1230448662784</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"367,-594 291,-594 291,-574 367,-574 367,-594\"/>\n",
       "<text text-anchor=\"middle\" x=\"329\" y=\"-580.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1230448662784&#45;&gt;1230448662736 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>1230448662784&#45;&gt;1230448662736</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M333.34,-573.67C336.81,-566.2 342.04,-554.95 347.8,-542.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"350.83,-544.35 351.87,-533.8 344.48,-541.39 350.83,-544.35\"/>\n",
       "</g>\n",
       "<!-- 1230448672336 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>1230448672336</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"379,-662 279,-662 279,-642 379,-642 379,-662\"/>\n",
       "<text text-anchor=\"middle\" x=\"329\" y=\"-648.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1230448672336&#45;&gt;1230448662784 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>1230448672336&#45;&gt;1230448662784</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329,-641.54C329,-632.23 329,-617.7 329,-605.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.5,-605.95 329,-595.95 325.5,-605.95 332.5,-605.95\"/>\n",
       "</g>\n",
       "<!-- 1230444484880 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>1230444484880</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"364,-736 294,-736 294,-704 364,-704 364,-736\"/>\n",
       "<text text-anchor=\"middle\" x=\"329\" y=\"-722.5\" font-family=\"monospace\" font-size=\"10.00\">W0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"329\" y=\"-710.5\" font-family=\"monospace\" font-size=\"10.00\"> (16, 8)</text>\n",
       "</g>\n",
       "<!-- 1230444484880&#45;&gt;1230448672336 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>1230444484880&#45;&gt;1230448672336</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329,-703.69C329,-694.8 329,-683.46 329,-673.77\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.5,-673.83 329,-663.83 325.5,-673.83 332.5,-673.83\"/>\n",
       "</g>\n",
       "<!-- 1230448669024 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>1230448669024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"130,-356 54,-356 54,-336 130,-336 130,-356\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-342.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 1230448669024&#45;&gt;1230448672720 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>1230448669024&#45;&gt;1230448672720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.32,-335.58C108.18,-326.68 120.77,-312.42 134.34,-297.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"136.95,-299.4 140.95,-289.59 131.7,-294.77 136.95,-299.4\"/>\n",
       "</g>\n",
       "<!-- 1230448664848 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>1230448664848</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"142,-478 42,-478 42,-458 142,-458 142,-478\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-464.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 1230448664848&#45;&gt;1230448669024 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>1230448664848&#45;&gt;1230448669024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92,-457.58C92,-438.31 92,-393.95 92,-367.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.5,-367.89 92,-357.89 88.5,-367.89 95.5,-367.89\"/>\n",
       "</g>\n",
       "<!-- 1230507151184 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>1230507151184</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"127,-600 57,-600 57,-568 127,-568 127,-600\"/>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-586.5\" font-family=\"monospace\" font-size=\"10.00\">W1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"92\" y=\"-574.5\" font-family=\"monospace\" font-size=\"10.00\"> (1, 16)</text>\n",
       "</g>\n",
       "<!-- 1230507151184&#45;&gt;1230448664848 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>1230507151184&#45;&gt;1230448664848</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92,-567.63C92,-547.51 92,-512.2 92,-489.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"95.5,-490 92,-480 88.5,-490 95.5,-490\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x11e7c76ce90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "y = model(x)\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()), show_attrs=True, show_saved=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlling the Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you don't want to store the computational graph. It's expensive (mostly memory). Always use `no_grad` when you don't need gradients, such as during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m     y \u001b[38;5;241m=\u001b[39m W(x)\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m----> 8\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# this will fail because we disabled the creation of the computational graph\u001b[39;00m\n",
      "File \u001b[1;32mD:\\miniforge\\envs\\791aa\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\miniforge\\envs\\791aa\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "W = nn.Linear(8, 16)\n",
    "x = torch.randn(1, 8)\n",
    "\n",
    "assert W.weight.grad is None\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = W(x).sum()\n",
    "y.backward()  # this will fail because we disabled the creation of the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "other times, you want to truncate the gradient at a certain point. For example, you can use it to freeze the first part of the network during finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367],\n",
       "        [ 1.2033, -0.1066,  0.3591, -0.1294, -0.9180, -0.3292,  0.2503, -0.0913,\n",
       "         -0.4861, -0.2739, -0.3371, -0.8888, -0.3838, -0.1801, -0.8955,  1.0367]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = nn.Linear(8, 16)\n",
    "W2 = nn.Linear(16, 16)\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "\n",
    "with torch.no_grad():\n",
    "    h = W1(x)  # no computational graph for first layer\n",
    "y = W2(h).sum()  # build computational graph. h is a root, so it will not be backpropagated into.\n",
    "\n",
    "y.backward()\n",
    "\n",
    "assert W1.weight.grad is None\n",
    "W2.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to disable the gradient only for some parameters, you can set `requires_grad = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = nn.Linear(8, 16)\n",
    "x = torch.randn(1, 8)\n",
    "W.weight.requires_grad = False\n",
    "\n",
    "y = W(x).sum()\n",
    "y.backward()\n",
    "\n",
    "assert W.weight.grad is None  # the gradient for the weight is not computed\n",
    "W.bias.grad  # the gradient for the bias is still computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you already computed the output and you don't want to backpropagate, you can call `detach` on the output. This method will \"detach\" the value from its computational graph, returning a new tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203],\n",
       "        [ 0.7221, -0.7408,  1.1143, -0.2721,  0.4590,  0.8037,  0.0920,  0.7366,\n",
       "          1.3422,  0.3162, -1.0225,  0.8792, -0.4370, -1.1245, -1.4430, -1.5203]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = nn.Linear(8, 16)\n",
    "W2 = nn.Linear(16, 16)\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "\n",
    "h = W1(x) \n",
    "h = h.detach() # no computational graph for first layer\n",
    "y = W2(h).sum()  # build computational graph. h is a root, so it will not be backpropagated into.\n",
    "y.backward()\n",
    "\n",
    "assert W1.weight.grad is None\n",
    "W2.weight.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch optimizers use the `grad` attribute to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "# you can pass a subset of the parameters if you want to \n",
    "opt = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# the gradient is accumulated in the .grad attribute, so you need to reset it every time.\n",
    "# zero_grad corresponds to:\n",
    "# [p.grad.zero_() for p in model.parameters()]\n",
    "opt.zero_grad()  \n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "y = model(x)\n",
    "y.backward()\n",
    "\n",
    "# the optimizer looks at the grad, compute the update direction, and updates each parameter.\n",
    "# For the basic SGD this corresponds to:\n",
    "# for p in model.parameters():\n",
    "#     g = p.grad\n",
    "#     p.data.sub_(opt.lr * g)  # operations ending with _ are in-place operations\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful vs Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have seen in the previous cells is the stateful API. Most methods are easily implemented with that API.\n",
    "However, when we need to manipulate the gradients of multiple tasks and backpropagate over the optimizer operations, the stateful API is quite clunky.\n",
    "Implementing MAML with the functional API is much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "\n",
    "def f(W):\n",
    "    return ((W ** 2)@x).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateful API:  [[-0.8199071288108826, 3.267639398574829]]\n"
     ]
    }
   ],
   "source": [
    "W.grad = None  # reset gradient (optimizer.zero_grad)\n",
    "l = f(W)\n",
    "l.backward()\n",
    "print(\"stateful API: \", W.grad.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of updating the `grad` attribute of each parameter, the functional method `grad` returns the gradient as an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional API:  [[-0.8199071288108826, 3.267639398574829]]\n"
     ]
    }
   ],
   "source": [
    "gw = grad(f)(W)\n",
    "print(\"functional API: \", gw.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need functional forward passes, because we will manipulate the parameters directly with separate SGD steps for each task and the stateful API is much more convenient.\n",
    "`functional_call` is the functional equivalent of the `forward` pass of a `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': tensor([[-0.2360, -0.5467, -0.1768],\n",
       "         [ 0.5111,  1.1632,  0.8960],\n",
       "         [ 0.1004,  0.1275,  0.6678]], grad_fn=<TBackward0>),\n",
       " 'bias': tensor([-0.0787,  0.4172,  0.7057], grad_fn=<ViewBackward0>)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.func import functional_call, grad\n",
    "\n",
    "x = torch.randn(4, 3)\n",
    "t = torch.randn(4, 3)\n",
    "model = nn.Linear(3, 3)\n",
    "\n",
    "def compute_loss(params, x, t):\n",
    "    y = functional_call(model, params, x)\n",
    "    return nn.functional.mse_loss(y, t)\n",
    "\n",
    "grad_weights = grad(compute_loss)(dict(model.named_parameters()), x, t)\n",
    "grad_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop over gradient operations\n",
    "\n",
    "we are going to see some toy examples of gradient computations.\n",
    "Here are the equations of the function and its derivatives so that you can check the results are correct:\n",
    "\n",
    "\\begin{aligned}\n",
    "& y=w^2 \\cdot x=\\left[\\begin{array}{l}\n",
    "w_1^2 x_1 \\\\\n",
    "w_1^2 x_2\n",
    "\\end{array}\\right] \\\\\n",
    "& \\frac{\\partial y}{\\partial w}=2 w x=\\left[\\begin{array}{c}\n",
    "2 w_1 x \\\\\n",
    "2 w_2 x\n",
    "\\end{array}\\right] \\\\\n",
    "& \\frac{\\partial y}{\\partial w_i \\partial w_j}=2 x=\\left[\\begin{array}{cc}\n",
    "2 x_1 & 0 \\\\\n",
    "0 & 2 x_2\n",
    "\\end{array}\\right]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfoo/dW:\t [[1.8623961210250854, -0.4453994035720825]]\n",
      "expect: \t [[1.8623961210250854, -0.4453994035720825]]\n",
      "\n",
      "Vector-Jacobian Product\n",
      "ones @ ddfoo/dW:\t [[-2.2551023960113525, 0.6777215600013733]]\n",
      "expect:          \t [-2.2551023960113525, 0.6777215600013733]\n",
      "\n",
      "Jacobian-Vector Product\n",
      "ddfoo/dW @ ones:\t [-2.2551023960113525, 0.6777215600013733]\n",
      "expect:          \t [-2.2551023960113525, 0.6777215600013733]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.func import vjp, jvp, grad\n",
    "\n",
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "ones = torch.tensor([1.0, 1.0]).reshape(1, 2)\n",
    "\n",
    "foo = lambda W: ((W ** 2)@x).sum()\n",
    "gw = grad(foo)(W)\n",
    "print(\"dfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", (2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "print(\"Vector-Jacobian Product\")\n",
    "_, jc = vjp(grad(foo), W)\n",
    "print(\"ones @ ddfoo/dW:\\t\", jc(ones)[0].tolist())\n",
    "print(\"expect:          \\t\", (2*x).tolist())\n",
    "print()\n",
    "\n",
    "print(\"Jacobian-Vector Product\")\n",
    "_, jc = jvp(grad(foo), (W,), (ones,))\n",
    "print(\"ddfoo/dW @ ones:\\t\", jc[0].tolist())\n",
    "print(\"expect:          \\t\", (2*x).tolist())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop over SGD step (full gradient and truncated)\n",
    "\n",
    "We are going to need the gradient over an SGD step to implement MAML.\n",
    "- We take a mini-batch and compute the gradient of the loss\n",
    "- update the model\n",
    "- compute the loss of the updated model\n",
    "- compute the gradient of the updated model w.r.t. initialization\n",
    "    - this operation requires backpropagation over the optimizer\n",
    "- there are two options:\n",
    "    - full gradient: the true gradient\n",
    "    - truncated gradient: the approximation where $\\nabla_{w} y$ is assumed constant when computing the gradient the second time\n",
    "\n",
    "SGD Step: $\\omega^*=\\omega-\\alpha \\nabla_w y=(1-a x) \\omega$\n",
    "\n",
    "model after SGD step: $y^*= w^* x^* = \\left(w-\\alpha \\nabla_{w y}\\right)^2 x^*=(1-2 \\alpha x)^2 w^2 x^*$\n",
    "\n",
    "Full gradient:\n",
    "$\\nabla_\\omega y^*=(1-2 \\alpha x)^2 2 \\omega x^*$\n",
    "\n",
    "Truncated Gradient (a.k.a. first-order approximation), which consider the result of the gradient as a constant:\n",
    "$F O \\nabla_w y^*= 2 \\omega x^*(1- 2 \\alpha x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:  [[1.5866552591323853, -0.07863344252109528]]\n",
      "x:  [-0.4648102819919586, -2.9824020862579346]\n",
      "\n",
      "dfoo/dW:\t [[-1.4749873876571655, 0.4690330922603607]]\n",
      "expect: \t [[-1.4749873876571655, 0.4690330922603607]]\n",
      "\n",
      "ddfoo/dW:\t [[-1.761969804763794, 1.1954480409622192]]\n",
      "expect: \t [[-1.7619696855545044, 1.1954479217529297]]\n",
      "\n",
      "FO-ddfoo/dW:\t [[-1.6121052503585815, 0.7488021850585938]]\n",
      "expect: \t [[-1.6121052503585815, 0.7488021850585938]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.func import vjp, jvp\n",
    "\n",
    "W = torch.randn(1, 2, requires_grad=True)\n",
    "x = torch.randn(2, requires_grad=True)\n",
    "ones = torch.tensor([1.0, 1.0]).reshape(1, 2)\n",
    "alpha = 0.1\n",
    "print(\"w: \", W.tolist())\n",
    "print(\"x: \", x.tolist())\n",
    "print()\n",
    "\n",
    "foo = lambda W: ((W ** 2)@x).sum()\n",
    "gw = grad(foo)(W)\n",
    "print(\"dfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", (2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "def foobar(W):\n",
    "    gw = grad(foo)(W)\n",
    "    wbar = W - alpha*gw\n",
    "    return foo(wbar)\n",
    "\n",
    "gw = grad(foobar)(W)\n",
    "print(\"ddfoo/dW:\\t\", gw.tolist())\n",
    "# print(\"dfoo/dW:\\t\", jc(ones)[0].tolist())\n",
    "print(\"expect: \\t\", (((1 -2*alpha*x) ** 2) * 2*W*x).tolist())\n",
    "print()\n",
    "\n",
    "wbar = W - alpha*grad(foo)(W).detach()\n",
    "gw = grad(foo)(wbar)\n",
    "print(\"FO-ddfoo/dW:\\t\", gw.tolist())\n",
    "print(\"expect: \\t\", ((1-alpha*2*x) * 2*W*x).tolist())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vmap\n",
    "vmap is a vectorization/batching transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.func import vmap\n",
    "batch_size, feature_size = 3, 5\n",
    "weights = torch.randn(feature_size, requires_grad=True)\n",
    "\n",
    "def model(feature_vec):\n",
    "    # Very simple linear model with activation\n",
    "    # note how this model computes the output for a single\n",
    "    # sample, not the whole minibatch\n",
    "    assert feature_vec.dim() == 1\n",
    "    return feature_vec.dot(weights).relu()\n",
    "\n",
    "examples = torch.randn(batch_size, feature_size)\n",
    "result = vmap(model)(examples)  # auto-vectorization with vmap\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use vmap to compute the gradient over multiple tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
