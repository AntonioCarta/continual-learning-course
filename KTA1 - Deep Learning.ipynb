{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a79a27e",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch\n",
    "\n",
    "we will use:\n",
    "- PyTorch for the deep learning tools (modules, optimizers, losses, ...)\n",
    "- torchvision for the computer vision architectures and datasets\n",
    "- Avalanche for the multi-task streams of datasets and dynamic architectures (and later for incremental learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58f322",
   "metadata": {},
   "source": [
    "install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ac545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: qpsolvers 4.3.1 does not provide the extra 'open-source-solvers'\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\w-32\\\\mambaforge\\\\envs\\\\791aa\\\\scripts\\\\torchrun.exe'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting avalanche-lib==0.5.0\n",
      "  Using cached avalanche_lib-0.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (4.9.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (5.9.8)\n",
      "Collecting gputil (from avalanche-lib==0.5.0)\n",
      "  Using cached GPUtil-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (1.4.1.post1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (3.8.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (1.26.4)\n",
      "Collecting pytorchcv (from avalanche-lib==0.5.0)\n",
      "  Using cached pytorchcv-0.0.67-py2.py3-none-any.whl.metadata (133 kB)\n",
      "Collecting wandb (from avalanche-lib==0.5.0)\n",
      "  Downloading wandb-0.16.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tensorboard>=1.15 (from avalanche-lib==0.5.0)\n",
      "  Using cached tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tqdm (from avalanche-lib==0.5.0)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (2.2.0)\n",
      "Collecting torchvision (from avalanche-lib==0.5.0)\n",
      "  Using cached torchvision-0.17.1-cp310-cp310-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchmetrics (from avalanche-lib==0.5.0)\n",
      "  Using cached torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting gdown (from avalanche-lib==0.5.0)\n",
      "  Using cached gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting qpsolvers[open_source_solvers] (from avalanche-lib==0.5.0)\n",
      "  Using cached qpsolvers-4.3.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting dill (from avalanche-lib==0.5.0)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from avalanche-lib==0.5.0) (23.2)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Downloading protobuf-5.26.0-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5.0) (69.0.3)\n",
      "Requirement already satisfied: six>1.9 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5.0) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Using cached werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from gdown->avalanche-lib==0.5.0) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from gdown->avalanche-lib==0.5.0) (3.13.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from gdown->avalanche-lib==0.5.0) (2.31.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from matplotlib->avalanche-lib==0.5.0) (2.8.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from scikit-learn->avalanche-lib==0.5.0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from scikit-learn->avalanche-lib==0.5.0) (3.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from torch->avalanche-lib==0.5.0) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from torch->avalanche-lib==0.5.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from torch->avalanche-lib==0.5.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from torch->avalanche-lib==0.5.0) (2024.2.0)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics->avalanche-lib==0.5.0)\n",
      "  Using cached lightning_utilities-0.10.1-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting torch (from avalanche-lib==0.5.0)\n",
      "  Using cached torch-2.2.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from tqdm->avalanche-lib==0.5.0) (0.4.6)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb->avalanche-lib==0.5.0)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->avalanche-lib==0.5.0)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb->avalanche-lib==0.5.0)\n",
      "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb->avalanche-lib==0.5.0)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from wandb->avalanche-lib==0.5.0) (6.0.1)\n",
      "Collecting setproctitle (from wandb->avalanche-lib==0.5.0)\n",
      "  Using cached setproctitle-1.3.3-cp310-cp310-win_amd64.whl.metadata (10 kB)\n",
      "Collecting appdirs>=1.4.3 (from wandb->avalanche-lib==0.5.0)\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting protobuf!=4.24.0,>=3.19.6 (from tensorboard>=1.15->avalanche-lib==0.5.0)\n",
      "  Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->avalanche-lib==0.5.0)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from requests[socks]->gdown->avalanche-lib==0.5.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from requests[socks]->gdown->avalanche-lib==0.5.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from requests[socks]->gdown->avalanche-lib==0.5.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from requests[socks]->gdown->avalanche-lib==0.5.0) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=1.15->avalanche-lib==0.5.0) (2.1.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from beautifulsoup4->gdown->avalanche-lib==0.5.0) (2.5)\n",
      "Collecting piqp>=0.2.2 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached piqp-0.2.4-cp310-cp310-win_amd64.whl.metadata (4.8 kB)\n",
      "Collecting daqp>=0.5.1 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached daqp-0.5.1-cp310-cp310-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting clarabel>=0.4.1 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Downloading clarabel-0.7.1-cp37-abi3-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting scs>=3.2.0 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached scs-3.2.4.post1-cp310-cp310-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting highspy>=1.1.2.dev3 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Downloading highspy-1.7.1.dev1-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting qpalm>=1.2.1 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached qpalm-1.2.2-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting ecos>=2.0.8 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached ecos-2.0.13-cp310-cp310-win_amd64.whl.metadata (8.2 kB)\n",
      "Collecting osqp>=0.6.2 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached osqp-0.6.5-cp310-cp310-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting proxsuite>=0.2.9 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached proxsuite-0.6.2-0-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting cvxopt>=1.2.6 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached cvxopt-1.3.2-cp310-cp310-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting quadprog>=0.1.11 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached quadprog-0.1.12-cp310-cp310-win_amd64.whl.metadata (1.3 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown->avalanche-lib==0.5.0)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from sympy->torch->avalanche-lib==0.5.0) (1.3.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->avalanche-lib==0.5.0)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting scipy>=1.2.0 (from qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached scipy-1.11.4-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting qdldl (from osqp>=0.6.2->qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached qdldl-0.1.7.post0-cp310-cp310-win_amd64.whl.metadata (1.8 kB)\n",
      "Collecting cmeel (from proxsuite>=0.2.9->qpsolvers[open_source_solvers]->avalanche-lib==0.5.0)\n",
      "  Using cached cmeel-0.53.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in c:\\users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages (from cmeel->proxsuite>=0.2.9->qpsolvers[open_source_solvers]->avalanche-lib==0.5.0) (2.0.1)\n",
      "Using cached avalanche_lib-0.5.0-py3-none-any.whl (971 kB)\n",
      "Using cached tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Using cached pytorchcv-0.0.67-py2.py3-none-any.whl (532 kB)\n",
      "Using cached torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "Using cached torchvision-0.17.1-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "Using cached torch-2.2.1-cp310-cp310-win_amd64.whl (198.6 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.2 MB 660.6 kB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.1/2.2 MB 656.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.2 MB 726.2 kB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.1/2.2 MB 602.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.2 MB 655.8 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/2.2 MB 577.4 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.2/2.2 MB 274.3 kB/s eta 0:00:08\n",
      "   --- ------------------------------------ 0.2/2.2 MB 270.6 kB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 295.5 kB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 0.2/2.2 MB 295.5 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 298.8 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 295.1 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 301.8 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 301.8 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 301.8 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 282.2 kB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 282.2 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/2.2 MB 268.8 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/2.2 MB 268.8 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/2.2 MB 268.8 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/2.2 MB 268.8 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.3/2.2 MB 268.8 kB/s eta 0:00:07\n",
      "   ------ --------------------------------- 0.4/2.2 MB 244.8 kB/s eta 0:00:08\n",
      "   ------ --------------------------------- 0.4/2.2 MB 244.0 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.4/2.2 MB 247.5 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.4/2.2 MB 247.5 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.4/2.2 MB 245.8 kB/s eta 0:00:08\n",
      "   ------- -------------------------------- 0.4/2.2 MB 242.8 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 0.4/2.2 MB 248.0 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 0.5/2.2 MB 247.2 kB/s eta 0:00:08\n",
      "   -------- ------------------------------- 0.5/2.2 MB 252.1 kB/s eta 0:00:07\n",
      "   -------- ------------------------------- 0.5/2.2 MB 254.5 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 255.7 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 262.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 0.5/2.2 MB 215.6 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 0.5/2.2 MB 215.6 kB/s eta 0:00:08\n",
      "   --------- ------------------------------ 0.5/2.2 MB 215.6 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 211.8 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 211.8 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 212.0 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 212.1 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 211.0 kB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 211.0 kB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 217.3 kB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 0.6/2.2 MB 219.6 kB/s eta 0:00:08\n",
      "   ------------ --------------------------- 0.7/2.2 MB 225.4 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 0.7/2.2 MB 238.9 kB/s eta 0:00:07\n",
      "   ------------- -------------------------- 0.7/2.2 MB 243.2 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 0.8/2.2 MB 244.1 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 0.8/2.2 MB 249.0 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 0.8/2.2 MB 258.3 kB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 272.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 277.8 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 289.2 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 1.0/2.2 MB 300.4 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 1.1/2.2 MB 308.0 kB/s eta 0:00:04\n",
      "   ------------------- -------------------- 1.1/2.2 MB 311.4 kB/s eta 0:00:04\n",
      "   -------------------- ------------------- 1.1/2.2 MB 311.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.2/2.2 MB 322.0 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 1.2/2.2 MB 330.5 kB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 340.2 kB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 348.4 kB/s eta 0:00:03\n",
      "   ------------------------ --------------- 1.4/2.2 MB 361.6 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 1.4/2.2 MB 373.4 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 1.5/2.2 MB 392.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 395.9 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 1.7/2.2 MB 417.1 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 1.7/2.2 MB 427.5 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 1.8/2.2 MB 452.9 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 467.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 484.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 511.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 517.4 kB/s eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Downloading grpcio-1.62.1-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.8 MB 3.6 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.3/3.8 MB 3.2 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.4/3.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.7/3.8 MB 3.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.8/3.8 MB 3.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.9/3.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.1/3.8 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 1.1/3.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.2/3.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.2/3.8 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.3/3.8 MB 2.1 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 1.4/3.8 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.4/3.8 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 1.5/3.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.6/3.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.8/3.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.8/3.8 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.9/3.8 MB 1.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.9/3.8 MB 970.3 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 1.9/3.8 MB 960.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.0/3.8 MB 953.5 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.0/3.8 MB 953.5 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.0/3.8 MB 935.7 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.1/3.8 MB 929.6 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.8 MB 923.9 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 2.1/3.8 MB 918.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.2/3.8 MB 930.9 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.2/3.8 MB 934.0 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 2.3/3.8 MB 932.1 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.3/3.8 MB 921.1 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.3/3.8 MB 906.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.3/3.8 MB 907.3 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 2.4/3.8 MB 897.2 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.4/3.8 MB 900.9 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 2.4/3.8 MB 896.7 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.5/3.8 MB 907.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.6/3.8 MB 910.4 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.6/3.8 MB 908.9 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.6/3.8 MB 901.1 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 2.7/3.8 MB 894.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.7/3.8 MB 897.7 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 2.7/3.8 MB 896.6 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.8/3.8 MB 891.6 kB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 2.8/3.8 MB 893.6 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.8/3.8 MB 894.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.8/3.8 MB 894.5 kB/s eta 0:00:02\n",
      "   ------------------------------ --------- 2.9/3.8 MB 886.4 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 886.2 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 895.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.1/3.8 MB 898.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.1/3.8 MB 903.5 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 900.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 606.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.2/3.8 MB 603.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.2/3.8 MB 568.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 3.2/3.8 MB 568.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 3.2/3.8 MB 568.5 kB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 3.2/3.8 MB 558.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 549.1 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 3.3/3.8 MB 548.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.3/3.8 MB 549.0 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 549.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 552.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.4/3.8 MB 552.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.5/3.8 MB 551.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.5/3.8 MB 554.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.5/3.8 MB 555.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 557.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 557.7 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 552.5 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 512.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.6/3.8 MB 511.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.6/3.8 MB 510.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.6/3.8 MB 510.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.6/3.8 MB 510.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 505.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 504.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 504.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.7/3.8 MB 502.3 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.7/3.8 MB 501.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.7/3.8 MB 501.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 498.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.8/3.8 MB 499.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 497.3 kB/s eta 0:00:00\n",
      "Using cached lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/105.4 kB ? eta -:--:--\n",
      "   --------------- ----------------------- 41.0/105.4 kB 487.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 102.4/105.4 kB 845.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 105.4/105.4 kB 674.8 kB/s eta 0:00:00\n",
      "Using cached protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
      "   ---------------------------------------- 0.0/263.5 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/263.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 71.7/263.5 kB 975.2 kB/s eta 0:00:01\n",
      "   --------------------- ------------------ 143.4/263.5 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 225.3/263.5 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 256.0/263.5 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 263.5/263.5 kB 1.2 MB/s eta 0:00:00\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Using cached qpsolvers-4.3.1-py3-none-any.whl (78 kB)\n",
      "Using cached setproctitle-1.3.3-cp310-cp310-win_amd64.whl (11 kB)\n",
      "Downloading clarabel-0.7.1-cp37-abi3-win_amd64.whl (321 kB)\n",
      "   ---------------------------------------- 0.0/321.5 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 61.4/321.5 kB 1.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 122.9/321.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 143.4/321.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 225.3/321.5 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ---------- 235.5/321.5 kB 424.3 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 256.0/321.5 kB 437.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 276.5/321.5 kB 437.2 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 286.7/321.5 kB 412.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 307.2/321.5 kB 404.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 321.5/321.5 kB 398.5 kB/s eta 0:00:00\n",
      "Using cached cvxopt-1.3.2-cp310-cp310-win_amd64.whl (12.8 MB)\n",
      "Using cached daqp-0.5.1-cp310-cp310-win_amd64.whl (81 kB)\n",
      "Using cached ecos-2.0.13-cp310-cp310-win_amd64.whl (72 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading highspy-1.7.1.dev1-cp310-cp310-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 259.2 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/1.7 MB 363.1 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/1.7 MB 363.1 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/1.7 MB 261.4 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/1.7 MB 261.4 kB/s eta 0:00:07\n",
      "   -- ------------------------------------- 0.1/1.7 MB 311.2 kB/s eta 0:00:06\n",
      "   -- ------------------------------------- 0.1/1.7 MB 312.9 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.2/1.7 MB 352.2 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.2/1.7 MB 406.0 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.2/1.7 MB 436.6 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.3/1.7 MB 462.0 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.3/1.7 MB 465.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.3/1.7 MB 498.8 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.4/1.7 MB 552.7 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 600.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.7 MB 629.6 kB/s eta 0:00:02\n",
      "   ------------ --------------------------- 0.6/1.7 MB 655.2 kB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.6/1.7 MB 712.5 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.7/1.7 MB 742.3 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 771.4 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.8/1.7 MB 600.0 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.8/1.7 MB 594.8 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.9/1.7 MB 604.1 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.9/1.7 MB 612.4 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.9/1.7 MB 607.6 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.7 MB 622.2 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 1.0/1.7 MB 616.7 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.0/1.7 MB 630.2 kB/s eta 0:00:02\n",
      "   ------------------------ --------------- 1.1/1.7 MB 625.0 kB/s eta 0:00:02\n",
      "   ------------------------- -------------- 1.1/1.7 MB 637.4 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.7 MB 649.6 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.2/1.7 MB 649.7 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 671.0 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 660.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 680.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 675.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.7 MB 533.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.7 MB 533.3 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.7 MB 525.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.7 MB 525.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.7 MB 512.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.5/1.7 MB 512.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 499.2 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.5/1.7 MB 498.3 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 499.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 499.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.6/1.7 MB 499.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 497.1 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 497.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.6/1.7 MB 501.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.7 MB 503.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.7 MB 503.1 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.7 MB 492.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 489.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 489.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 482.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 478.0 kB/s eta 0:00:00\n",
      "Using cached osqp-0.6.5-cp310-cp310-win_amd64.whl (293 kB)\n",
      "Using cached scipy-1.11.4-cp310-cp310-win_amd64.whl (44.1 MB)\n",
      "Using cached piqp-0.2.4-cp310-cp310-win_amd64.whl (884 kB)\n",
      "Using cached proxsuite-0.6.2-0-cp310-cp310-win_amd64.whl (7.3 MB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached qpalm-1.2.2-cp310-cp310-win_amd64.whl (151 kB)\n",
      "Using cached quadprog-0.1.12-cp310-cp310-win_amd64.whl (91 kB)\n",
      "Using cached scs-3.2.4.post1-cp310-cp310-win_amd64.whl (8.4 MB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached cmeel-0.53.3-py3-none-any.whl (20 kB)\n",
      "Using cached qdldl-0.1.7.post0-cp310-cp310-win_amd64.whl (85 kB)\n",
      "Installing collected packages: gputil, daqp, appdirs, werkzeug, tqdm, tensorboard-data-server, smmap, setproctitle, sentry-sdk, scipy, quadprog, PySocks, protobuf, piqp, markdown, lightning-utilities, highspy, grpcio, docker-pycreds, dill, cvxopt, cmeel, Click, absl-py, torch, tensorboard, scs, qpsolvers, qpalm, qdldl, pytorchcv, proxsuite, gitdb, ecos, clarabel, torchvision, torchmetrics, osqp, GitPython, gdown, wandb, avalanche-lib\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.12.0\n",
      "    Uninstalling scipy-1.12.0:\n",
      "      Successfully uninstalled scipy-1.12.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n"
     ]
    }
   ],
   "source": [
    "!pip install avalanche-lib==0.5.0 pytorchcv==0.0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7e729",
   "metadata": {},
   "source": [
    "tested with:\n",
    "- python 3.10\n",
    "- avalanche 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad2b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import avalanche\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9648a",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "first, we load the data.\n",
    "- The data is automatically downloaded the first time.\n",
    "- `n_experiences` defines the lenght of the stream. Today we train our model offline, so we set `n_experiences=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b171887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\w-32\\mambaforge\\envs\\791aa\\lib\\site-packages\\avalanche\\benchmarks\\scenarios\\supervised.py:389: UserWarning: stream generator will be converted to a list.\n",
      "  warnings.warn(\"stream generator will be converted to a list.\")\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitCIFAR10\n",
    "from avalanche.benchmarks import benchmark_with_validation_stream\n",
    "\n",
    "# data is stored in $HOME/.avalanche - you can change this setting in $HOME/.avalanche/config.json\n",
    "benchmark = SplitCIFAR10(n_experiences=1)\n",
    "benchmark = benchmark_with_validation_stream(benchmark, validation_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b4f21",
   "metadata": {},
   "source": [
    "we take the data from the streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31c41404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = benchmark.train_stream[0].dataset\n",
    "valid_data = benchmark.valid_stream[0].dataset\n",
    "test_data = benchmark.test_stream[0].dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11708f64",
   "metadata": {},
   "source": [
    "NOTE: colors are wrong due to augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fbe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label=0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGzCAYAAABpdMNsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAj00lEQVR4nO3df3RU5b3v8c+AZAiSDERCkikhBig/5JerKYaIIkIKxHM9IFh/1FNDdYFgoMWUoulVUNpzotBataVg27Pg6C0iuASWXotFCLHYgCWQAioR0lBiSYLSMhMCGWjy3D+scx0Jkp3M8GTC+7XWs1Zm7+/s+W62az7u2XuecRljjAAAuMQ62W4AAHB5IoAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIBwWVu9erVcLpeOHDni6Hnjxo3TsGHDwtrL1VdfrRkzZoR1m0B7RgABHVAgENDDDz8sr9er2NhYZWZmasuWLbbbAkIQQEAHNGPGDD399NO655579Oyzz6pz58665ZZbtGPHDtutAUFX2G4AQHi9++67Wrt2rZYtW6YFCxZIku69914NGzZMCxcu1B//+EfLHQKf4gwI+JxNmzbp3/7t3+T1euV2u9W/f3/96Ec/UmNjY7P1paWluv766xUbG6v09HStXLnyvJpAIKDFixdrwIABcrvdSk1N1cKFCxUIBCKyD6+88oo6d+6sWbNmBZd17dpV999/v0pKSlRVVRWR1wWc4gwI+JzVq1ere/fuys/PV/fu3bVt2zYtWrRIfr9fy5YtC6n9xz/+oVtuuUV33HGH7r77bq1bt05z5sxRTEyM7rvvPklSU1OT/v3f/107duzQrFmzNGTIEO3fv18/+9nP9OGHH2rjxo0X7KWpqUl///vfW9S3x+NRly5dJEl79+7VwIEDFR8fH1Jz3XXXSZLKysqUmpra0n8SIHIMcBlbtWqVkWQqKyuNMcacPn36vJoHHnjAdOvWzTQ0NASX3XTTTUaS+elPfxpcFggEzLXXXmt69+5tzp49a4wx5sUXXzSdOnUyf/jDH0K2uXLlSiPJvPPOO8FlaWlpJjc3N/i4srLSSGrRKCoqCj5v6NChZvz48eftx3vvvWckmZUrVzr6NwIihTMg4HNiY2ODf9fV1SkQCOjGG2/U888/r4MHD2rkyJHB9VdccYUeeOCB4OOYmBg98MADmjNnjkpLSzV69GitX79eQ4YM0eDBg/XJJ58Ea8ePHy9JKioq0vXXX99sL8nJyS2+c+3zfZ05c0Zut/u8mq5duwbXA+0BAQR8znvvvadHH31U27Ztk9/vD1nn8/lCHnu9Xl155ZUhywYOHChJOnLkiEaPHq1Dhw7pgw8+UGJiYrOvd/z48Qv20rVrV2VnZzveh9jY2GavLzU0NATXA+0BAQT8y8mTJ3XTTTcpPj5eS5YsUf/+/dW1a1ft2bNHDz/8sJqamhxvs6mpScOHD9fTTz/d7PovuxbT2Niojz/+uEWvk5CQoJiYGElSSkqK/va3v51XU11dLenT4ATaAwII+Jft27frxIkTevXVVzV27Njg8srKymbrjx07pvr6+pCzoA8//FDSp7MaSFL//v315z//WRMmTJDL5XLUT1VVldLT01tUW1RUpHHjxkmSrr32WhUVFcnv94fciLBr167geqA9IICAf+ncubMkyRgTXHb27Fn98pe/bLb+n//8p55//nnl5+cHa59//nklJiYqIyNDknTHHXfojTfe0K9//euQ26KlT6/FNDU1nfcx3mdaew3o9ttv109+8hP96le/Cn4PKBAIaNWqVcrMzOQOOLQbBBDwL9dff7169uyp3Nxcffe735XL5dKLL74YEkif5/V69dRTT+nIkSMaOHCgXn75ZZWVlelXv/pV8Jbob3/721q3bp1mz56toqIijRkzRo2NjTp48KDWrVunN998U1//+teb3X5rrwFlZmbqm9/8pgoKCnT8+HENGDBA//M//6MjR47ov//7vx1vD4gY27fhATZ98Tbsd955x4wePdrExsYar9drFi5caN58883zbnW+6aabzNChQ83u3btNVlaW6dq1q0lLSzO/+MUvznuNs2fPmqeeesoMHTrUuN1u07NnT5ORkWGeeOIJ4/P5gnVfvA27Lc6cOWMWLFhgkpOTjdvtNqNGjTKbN28Oy7aBcHEZc4H/vQMAIIKYigcAYAUBBACwggACAFhBAAEArCCAAABWEEAAACva3RdRm5qadOzYMcXFxTmeugQAYJ8xRnV1dfJ6verU6cLnOe0ugI4dO8ZUIQDQAVRVValPnz4XXN/uPoKLi4uz3QIAIAwu9n4esQBavny5rr76anXt2lWZmZl69913W/Q8PnYDgI7hYu/nEQmgl19+Wfn5+Vq8eLH27NmjkSNHatKkSV/641sAgMtLROaCy8zM1KhRo/SLX/xC0qc3FqSmpmrevHl65JFHQmoDgUDIrzf6/X6uAQFAB+Dz+UJ+k+qLwn4GdPbsWZWWloZMI9+pUydlZ2erpKTkvPrCwkJ5PJ7gIHwA4PIQ9gD65JNP1NjYqKSkpJDlSUlJqqmpOa++oKBAPp8vOKqqqsLdEgCgHbJ+G7bb7Zbb7bbdBgDgEgv7GVCvXr3UuXNn1dbWhiyvra1VcnJyuF8OABClwh5AMTExysjI0NatW4PLmpqatHXrVmVlZYX75QAAUSoiH8Hl5+crNzdXX//613XdddfpmWeeUX19vb7zne9E4uUAAFEoIgF055136uOPP9aiRYtUU1Oja6+9Vps3bz7vxgQAwOUrIt8Dagu/3y+Px2O7DQBAG13y7wEBANASBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAV1n+O4UIu9g1aAEDbuFwuq6/PGRAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCi3c4FBwBf5kB1y2sPHqx3tvEuzspvv+FKZ0+AJM6AAACWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACuYigewwOVy2W4hyBhjuwVJ0jsHndXPmrmgxbXv79njbOOnP3FU/oMlz7S4dulj45310oFxBgQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxgLjggDF74v/W2W2i1//3czhbX/ud3R0esj5fWvO6o/v0dP41QJ84tWzShxbVjbv6Ho21PuaGHw26iB2dAAAArwh5Ajz/+uFwuV8gYPHhwuF8GABDlIvIR3NChQ/XWW2/9/xe5gk/6AAChIpIMV1xxhZKTkyOxaQBABxGRa0CHDh2S1+tVv379dM899+jo0aMXrA0EAvL7/SEDANDxhT2AMjMztXr1am3evFkrVqxQZWWlbrzxRtXV1TVbX1hYKI/HExypqanhbgkA0A6FPYBycnL0zW9+UyNGjNCkSZP0xhtv6OTJk1q3bl2z9QUFBfL5fMFRVVUV7pYAAO1QxO8O6NGjhwYOHKjDhw83u97tdsvtdke6DQBAOxPx7wGdOnVKFRUVSklJifRLAQCiSNgDaMGCBSouLtaRI0f0xz/+Ubfddps6d+6su+++O9wvBQCIYmH/CO6jjz7S3XffrRMnTigxMVE33HCDdu7cqcTExHC/FNCOnLPdQKv91/f+o8W1//nd5j9KD4cdO96J2Lbbk6k3fsVRvTHRO83TxYQ9gNauXRvuTQIAOiDmggMAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsiPjPMSA8Xij6wFH9vTcPiVAnaE7u/7rVdgttUGG7AUmSx+Ox3cIlctpR9bcX/LrFtS/+ZKbTZqziDAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwgql4LKp2UJs7/hpH207Zd6rFtd8YfqWjbaM5O2w3cEkcdDaLjAZ3a3mtxxPvbOOXif/z01ktrn38MabiAQDgogggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwArmgrMoJYLbnjiie4trjTER7CR6rVx/2HYL7c6USQsc1Zf/4Sctrj137pzTdvAF48Y/aLsFRzgDAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVjAXXJRwOl+by+WKSG1reolWr6zfZLuFdufDHT91VP/G/pbPBfeXir84bQdf8NGeFbZbcIQzIACAFY4D6O2339att94qr9crl8uljRs3hqw3xmjRokVKSUlRbGyssrOzdejQoXD1CwDoIBwHUH19vUaOHKnly5c3u37p0qV67rnntHLlSu3atUtXXnmlJk2apIaGhjY3CwDoOBxfA8rJyVFOTk6z64wxeuaZZ/Too49qypQpkqQXXnhBSUlJ2rhxo+666662dQsA6DDCeg2osrJSNTU1ys7ODi7zeDzKzMxUSUlJs88JBALy+/0hAwDQ8YU1gGpqaiRJSUlJIcuTkpKC676osLBQHo8nOFJTU8PZEgCgnbJ+F1xBQYF8Pl9wVFVV2W4JAHAJhDWAkpOTJUm1tbUhy2tra4Prvsjtdis+Pj5kAAA6vrAGUHp6upKTk7V169bgMr/fr127dikrKyucLwUAiHKO74I7deqUDh8+HHxcWVmpsrIyJSQkqG/fvpo/f75+/OMf66tf/arS09P12GOPyev1aurUqeHsGwAQ5RwH0O7du3XzzTcHH+fn50uScnNztXr1ai1cuFD19fWaNWuWTp48qRtuuEGbN29W165dw9d1O1Vx2ll9/26R6SPSXImjHdXP//68Ftc+U/AfTttBO3bXbfe0uLauYlcEO0F75DiAxo0b96VzgblcLi1ZskRLlixpU2MAgI7N+l1wAIDLEwEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALDCZb5sXh0L/H6/PB6PfD5fu/hpBpfLZbsFAIhKF3sf5wwIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsOIK2w20d8+t+1uLa797x1ci2AkAdCycAQEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACtcxhhju4nP8/v98ng88vl8io+Pt91Ou3HtjU86qv/zjoIIdQIALXOx93HOgAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArmIoHcrlctlsA0AExFQ8AoF0igAAAVjgOoLffflu33nqrvF6vXC6XNm7cGLJ+xowZcrlcIWPy5Mnh6hcA0EE4DqD6+nqNHDlSy5cvv2DN5MmTVV1dHRwvvfRSm5oEAHQ8Vzh9Qk5OjnJycr60xu12Kzk5udVNAQA6vohcA9q+fbt69+6tQYMGac6cOTpx4sQFawOBgPx+f8gAAHR8YQ+gyZMn64UXXtDWrVv11FNPqbi4WDk5OWpsbGy2vrCwUB6PJzhSU1PD3RIAoB1q0/eAXC6XNmzYoKlTp16w5i9/+Yv69++vt956SxMmTDhvfSAQUCAQCD72+/1KTU3le0CXEN8DAhAJ1r8H1K9fP/Xq1UuHDx9udr3b7VZ8fHzIAAB0fBEPoI8++kgnTpxQSkpKpF8KABBFHN8Fd+rUqZCzmcrKSpWVlSkhIUEJCQl64oknNH36dCUnJ6uiokILFy7UgAEDNGnSpLA2DgCIcsahoqIiI+m8kZuba06fPm0mTpxoEhMTTZcuXUxaWpqZOXOmqampafH2fT6fkWR8Pp/T1tBKzR1PBoPBaOu42Ps4k5GCmxAARIT1mxAAAGgOAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsMLxZKSw48e/2eao/rHvLY5QJwAQHpwBAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYwFY9FLpfLdgsAYA1nQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKxwFUGFhoUaNGqW4uDj17t1bU6dOVXl5eUhNQ0OD8vLydNVVV6l79+6aPn26amtrw9o0ACD6OQqg4uJi5eXlaefOndqyZYvOnTuniRMnqr6+Pljz0EMP6bXXXtP69etVXFysY8eOadq0aWFvHAAQ5UwbHD9+3EgyxcXFxhhjTp48abp06WLWr18frPnggw+MJFNSUtLsNhoaGozP5wuOqqoqI8n4fL62tBYVJDEYDEaHHRd7H2/TNSCfzydJSkhIkCSVlpbq3Llzys7ODtYMHjxYffv2VUlJSbPbKCwslMfjCY7U1NS2tAQAiBKtDqCmpibNnz9fY8aM0bBhwyRJNTU1iomJUY8ePUJqk5KSVFNT0+x2CgoK5PP5gqOqqqq1LQEAosgVrX1iXl6eDhw4oB07drSpAbfbLbfb3aZtAACiT6vOgObOnavXX39dRUVF6tOnT3B5cnKyzp49q5MnT4bU19bWKjk5uU2NAgA6FkcBZIzR3LlztWHDBm3btk3p6ekh6zMyMtSlSxdt3bo1uKy8vFxHjx5VVlZWeDoGAHQIjj6Cy8vL05o1a7Rp0ybFxcUFr+t4PB7FxsbK4/Ho/vvvV35+vhISEhQfH6958+YpKytLo0ePjsgOAACiVDhuG161alWw5syZM+bBBx80PXv2NN26dTO33Xabqa6ubvFr+Hy+Ft2+d7m50L89g8FgtNdxsfdx17/e3NoNv98vj8cjn8+n+Ph42+20Gy6Xy3YLAODIxd7HmQsOAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGBFq3+OAZeW0wkrXig62+La3PH8HAaikcdB7WCH297lsB6twRkQAMAKAggAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwgrngOqh7b45pea3DeebuW7TBUf2qH01zVA+0TN8WV/6w8L8cbfn3v9/iqP4vFRUtrv370f2Oti0ddFgfPTgDAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKxwGeNwHpYI8/v98ng88vl8io+Pt90OwuDA0ZbXDk9zRa4RdDC9WlxZbz52tOU3fn/SUf3Bgy2fLufAfmdT8ezds7fFtR866EOSdLrIWb1DF3sf5wwIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYwVxwiGoHqlteO9w7xOHWHc6rhXYr0m9zPge15Q7/s9qz53CLa53OM7f8yWnOmnGIueAAAO2SowAqLCzUqFGjFBcXp969e2vq1KkqLy8PqRk3bpxcLlfImD17dlibBgBEP0cBVFxcrLy8PO3cuVNbtmzRuXPnNHHiRNXX14fUzZw5U9XV1cGxdOnSsDYNAIh+Vzgp3rx5c8jj1atXq3fv3iotLdXYsWODy7t166bk5OTwdAgA6JDadA3I5/v00ltCQkLI8t/+9rfq1auXhg0bpoKCAp0+ffqC2wgEAvL7/SEDANDxOToD+rympibNnz9fY8aM0bBhw4LLv/WtbyktLU1er1f79u3Tww8/rPLycr366qvNbqewsFBPPPFEa9sAAESpVgdQXl6eDhw4oB07doQsnzVrVvDv4cOHKyUlRRMmTFBFRYX69+9/3nYKCgqUn58ffOz3+5WamtratgAAUaJVATR37ly9/vrrevvtt9WnT58vrc3MzJQkHT58uNkAcrvdcrvdrWkDABDFHAWQMUbz5s3Thg0btH37dqWnp1/0OWVlZZKklJSUVjUIAOiYHAVQXl6e1qxZo02bNikuLk41NTWSJI/Ho9jYWFVUVGjNmjW65ZZbdNVVV2nfvn166KGHNHbsWI0YMSIiOwAAiE6OAmjFihWSPv2y6eetWrVKM2bMUExMjN566y0988wzqq+vV2pqqqZPn65HH300bA0DADoG5oIDLuAdB3N23TDEFblG0Gbt7G0uYqov/I2XZnmvjOx/t8wFBwBolwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVrf49IKCjGzO45bV/dTjVS5qLqXsQfindbHfgDGdAAAArCCAAgBUEEADACgIIAGAFAQQAsIIAAgBYQQABAKwggAAAVhBAAAArCCAAgBUEEADACuaCA8Kgr8N643DuOJdriIPqg86aASzhDAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwgql4gChgzAcR2/aW/S2vnTjCFbE+cPnhDAgAYAUBBACwggACAFhBAAEArCCAAABWEEAAACsIIACAFQQQAMAKAggAYAUBBACwggACAFjBXHDAZe4bw1tea4yJWB8ulzdi20b7xBkQAMAKRwG0YsUKjRgxQvHx8YqPj1dWVpZ+97vfBdc3NDQoLy9PV111lbp3767p06ertrY27E0DAKKfowDq06ePnnzySZWWlmr37t0aP368pkyZovfee0+S9NBDD+m1117T+vXrVVxcrGPHjmnatGkRaRwAEN1cpo0f6iYkJGjZsmW6/fbblZiYqDVr1uj222+XJB08eFBDhgxRSUmJRo8e3aLt+f1+eTwe+Xw+xcfHt6U1AFEkkteAjDkWsW1HM5crsr/vdLH38VZfA2psbNTatWtVX1+vrKwslZaW6ty5c8rOzg7WDB48WH379lVJSckFtxMIBOT3+0MGAKDjcxxA+/fvV/fu3eV2uzV79mxt2LBB11xzjWpqahQTE6MePXqE1CclJammpuaC2yssLJTH4wmO1NRUxzsBAIg+jgNo0KBBKisr065duzRnzhzl5ubq/fffb3UDBQUF8vl8wVFVVdXqbQEAoofj7wHFxMRowIABkqSMjAz96U9/0rPPPqs777xTZ8+e1cmTJ0POgmpra5WcnHzB7bndbrndbuedAwCiWpu/B9TU1KRAIKCMjAx16dJFW7duDa4rLy/X0aNHlZWV1daXAQB0MI7OgAoKCpSTk6O+ffuqrq5Oa9as0fbt2/Xmm2/K4/Ho/vvvV35+vhISEhQfH6958+YpKyurxXfAAQAuH44C6Pjx47r33ntVXV0tj8ejESNG6M0339Q3vvENSdLPfvYzderUSdOnT1cgENCkSZP0y1/+MiKNA+hYuFX68tPm7wGFG98DAoBLI2q/BwQAQFsQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWEEAAQCsIIAAAFY4ng070j6bmIEfpgOA6HaxiXbaXQDV1dVJEj9MBwBRrq6uTh6P54Lr291ccE1NTTp27Jji4uJC5iny+/1KTU1VVVVVh54jjv3sOC6HfZTYz44mHPtpjFFdXZ28Xq86dbrwlZ52dwbUqVMn9enT54Lr4+PjO/TB/wz72XFcDvsosZ8dTVv388vOfD7DTQgAACsIIACAFVETQG63W4sXL5bb7bbdSkSxnx3H5bCPEvvZ0VzK/Wx3NyEAAC4PUXMGBADoWAggAIAVBBAAwAoCCABgBQEEALAiagJo+fLluvrqq9W1a1dlZmbq3Xfftd1SWD3++ONyuVwhY/DgwbbbapO3335bt956q7xer1wulzZu3Biy3hijRYsWKSUlRbGxscrOztahQ4fsNNsGF9vPGTNmnHdsJ0+ebKfZViosLNSoUaMUFxen3r17a+rUqSovLw+paWhoUF5enq666ip1795d06dPV21traWOW6cl+zlu3Ljzjufs2bMtddw6K1as0IgRI4KzHWRlZel3v/tdcP2lOpZREUAvv/yy8vPztXjxYu3Zs0cjR47UpEmTdPz4cduthdXQoUNVXV0dHDt27LDdUpvU19dr5MiRWr58ebPrly5dqueee04rV67Url27dOWVV2rSpElqaGi4xJ22zcX2U5ImT54ccmxfeumlS9hh2xUXFysvL087d+7Uli1bdO7cOU2cOFH19fXBmoceekivvfaa1q9fr+LiYh07dkzTpk2z2LVzLdlPSZo5c2bI8Vy6dKmljlunT58+evLJJ1VaWqrdu3dr/PjxmjJlit577z1Jl/BYmihw3XXXmby8vODjxsZG4/V6TWFhocWuwmvx4sVm5MiRttuIGElmw4YNwcdNTU0mOTnZLFu2LLjs5MmTxu12m5deeslCh+Hxxf00xpjc3FwzZcoUK/1EyvHjx40kU1xcbIz59Nh16dLFrF+/PljzwQcfGEmmpKTEVptt9sX9NMaYm266yXzve9+z11SE9OzZ0/zmN7+5pMey3Z8BnT17VqWlpcrOzg4u69Spk7Kzs1VSUmKxs/A7dOiQvF6v+vXrp3vuuUdHjx613VLEVFZWqqamJuS4ejweZWZmdrjjKknbt29X7969NWjQIM2ZM0cnTpyw3VKb+Hw+SVJCQoIkqbS0VOfOnQs5noMHD1bfvn2j+nh+cT8/89vf/la9evXSsGHDVFBQoNOnT9toLywaGxu1du1a1dfXKysr65Iey3Y3G/YXffLJJ2psbFRSUlLI8qSkJB08eNBSV+GXmZmp1atXa9CgQaqurtYTTzyhG2+8UQcOHFBcXJzt9sKupqZGkpo9rp+t6ygmT56sadOmKT09XRUVFfrhD3+onJwclZSUqHPnzrbbc6ypqUnz58/XmDFjNGzYMEmfHs+YmBj16NEjpDaaj2dz+ylJ3/rWt5SWliav16t9+/bp4YcfVnl5uV599VWL3Tq3f/9+ZWVlqaGhQd27d9eGDRt0zTXXqKys7JIdy3YfQJeLnJyc4N8jRoxQZmam0tLStG7dOt1///0WO0Nb3XXXXcG/hw8frhEjRqh///7avn27JkyYYLGz1snLy9OBAwei/hrlxVxoP2fNmhX8e/jw4UpJSdGECRNUUVGh/v37X+o2W23QoEEqKyuTz+fTK6+8otzcXBUXF1/SHtr9R3C9evVS586dz7sDo7a2VsnJyZa6irwePXpo4MCBOnz4sO1WIuKzY3e5HVdJ6tevn3r16hWVx3bu3Ll6/fXXVVRUFPK7XcnJyTp79qxOnjwZUh+tx/NC+9mczMxMSYq64xkTE6MBAwYoIyNDhYWFGjlypJ599tlLeizbfQDFxMQoIyNDW7duDS5ramrS1q1blZWVZbGzyDp16pQqKiqUkpJiu5WISE9PV3Jycshx9fv92rVrV4c+rpL00Ucf6cSJE1F1bI0xmjt3rjZs2KBt27YpPT09ZH1GRoa6dOkScjzLy8t19OjRqDqeF9vP5pSVlUlSVB3P5jQ1NSkQCFzaYxnWWxoiZO3atcbtdpvVq1eb999/38yaNcv06NHD1NTU2G4tbL7//e+b7du3m8rKSvPOO++Y7Oxs06tXL3P8+HHbrbVaXV2d2bt3r9m7d6+RZJ5++mmzd+9e89e//tUYY8yTTz5pevToYTZt2mT27dtnpkyZYtLT082ZM2csd+7Ml+1nXV2dWbBggSkpKTGVlZXmrbfeMl/72tfMV7/6VdPQ0GC79RabM2eO8Xg8Zvv27aa6ujo4Tp8+HayZPXu26du3r9m2bZvZvXu3ycrKMllZWRa7du5i+3n48GGzZMkSs3v3blNZWWk2bdpk+vXrZ8aOHWu5c2ceeeQRU1xcbCorK82+ffvMI488Ylwul/n9739vjLl0xzIqAsgYY37+85+bvn37mpiYGHPdddeZnTt32m4prO68806TkpJiYmJizFe+8hVz5513msOHD9tuq02KioqMpPNGbm6uMebTW7Efe+wxk5SUZNxut5kwYYIpLy+323QrfNl+nj592kycONEkJiaaLl26mLS0NDNz5syo+5+n5vZPklm1alWw5syZM+bBBx80PXv2NN26dTO33Xabqa6uttd0K1xsP48ePWrGjh1rEhISjNvtNgMGDDA/+MEPjM/ns9u4Q/fdd59JS0szMTExJjEx0UyYMCEYPsZcumPJ7wEBAKxo99eAAAAdEwEEALCCAAIAWEEAAQCsIIAAAFYQQAAAKwggAIAVBBAAwAoCCABgBQEEALCCAAIAWPH/AJPRF34/82grAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_data[0][0].transpose(0,2).numpy())\n",
    "plt.title(f\"label={train_data[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b374b17",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e71f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([100, 3, 32, 32])\n",
      "y: tensor([6, 4, 5, 3, 0, 1, 5, 5, 5, 6, 6, 1, 4, 1, 1, 3, 4, 4, 2, 1, 0, 0, 5, 0,\n",
      "        6, 2, 5, 1, 1, 1, 4, 0, 0, 1, 4, 6, 2, 2, 0, 4, 3, 1, 2, 0, 5, 5, 0, 2,\n",
      "        2, 6, 4, 5, 3, 5, 2, 5, 0, 4, 0, 6, 1, 0, 6, 3, 2, 3, 3, 6, 1, 6, 6, 3,\n",
      "        6, 6, 6, 0, 4, 1, 5, 6, 5, 2, 6, 3, 2, 6, 4, 4, 5, 6, 3, 4, 5, 1, 2, 5,\n",
      "        1, 6, 4, 1])\n",
      "t: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "0, 100, 200, 300, "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "for i, (x, y, t) in enumerate(dataloader):\n",
    "    # x is the input\n",
    "    # y is the target class\n",
    "    # t is the task label.\n",
    "    # We will use it for multi-task problems\n",
    "    pass\n",
    "    if i == 0:\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y: {y}\")\n",
    "        print(f\"t: {t}\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218664fc",
   "metadata": {},
   "source": [
    "# Model\n",
    "We use a resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60eeb7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the slimmed ResNet as used by Lopez et al. in the GEM paper.\"\"\"\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "from avalanche.models import DynamicModule\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(0, len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i < (len(sizes) - 2):\n",
    "                layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def SlimResNet18(nclasses, nf=20):\n",
    "    \"\"\"Slimmed ResNet18.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f67c27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=160, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SlimResNet18(nclasses=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d5923",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9871: 100%|| 1094/1094 [04:22<00:00,  4.16it/s]\n",
      "Loss: 0.7937: 100%|| 1094/1094 [06:57<00:00,  2.62it/s]\n",
      "Loss: 0.6939: 100%|| 1094/1094 [05:19<00:00,  3.42it/s]\n",
      "Loss: 0.6995: 100%|| 1094/1094 [05:51<00:00,  3.11it/s]\n",
      "Loss: 0.5157:  71%|  | 782/1094 [04:26<03:42,  1.40it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cpu'  # do yourself a favor and use a gpu by setting device='cuda'\n",
    "model = SlimResNet18(nclasses=10)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.04)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Avalanche datasets have a training and eval mode (train/eval methods)\n",
    "# to activate different augmentations\n",
    "# don't forget to activate it!\n",
    "train_data = train_data.train()\n",
    "\n",
    "# Iterate over the dataset and train the model\n",
    "model.train()  # don't forget to set the training mode!\n",
    "for ep in range(10):  # 10 epoch is a bit on the low end. With a GPU you can run a larger experiment\n",
    "    dataloader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0)  # increase num_workers if you have more CPUs\n",
    "    pbar = tqdm(dataloader)\n",
    "    for (x, y, _) in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # the order matters!\n",
    "        # - reset the gradients\n",
    "        # - forward pass\n",
    "        # - backward pass\n",
    "        # - descent step\n",
    "        optimizer.zero_grad()   \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Loss: {loss.item():0.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017e21f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACC: 0.8499: 100%|| 469/469 [00:32<00:00, 14.34it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "train_data = valid_data.eval()\n",
    "\n",
    "dataloader = DataLoader(valid_data, batch_size=32, num_workers=0)  # increase num_workers if you have more CPUs\n",
    "pbar = tqdm(dataloader)\n",
    "correct, tot = 0, 0\n",
    "for (x, y, _) in pbar:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    pred = model(x)\n",
    "    _, pred = torch.max(pred.data, 1)\n",
    "    correct += (pred == y).sum().item()\n",
    "    tot += x.shape[0]\n",
    "    pbar.set_description(f\"ACC: {correct / tot:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281538b",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Pick a couple of them to experiment with:\n",
    "\n",
    "- **resnet**: check the ResNet code. Do you understand everything? If not, check the documentation or ask for help.\n",
    "    - advanced version: now change the input transformations to change the input size of the images. Are you able to change the resnet architecture to make it compatible with the new image? Any change to the architecture is fine (just don't pad/crop the image...).\n",
    "- **dataloading**: If you have a large GPU (V100/A100), you need to feed the data fast enough, otherwise you are going to waste precious GPU cycles. Transformations can be expensive and dataloaders can make a big difference in performance.\n",
    "    - simple exercise: test and profile different augmentations from [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html). Play with the DataLoader parameters and measure its performance ([doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Try varying `num_workers`, `pin_memory`, and any other argument that you feel is relevant. Profile your code by timing the loading of the entire dataset in mini-batches, like you would do for a training epoch (without the forward/backward pass).\n",
    "    - advanced exercise: you can also try [FFCV](https://github.com/libffcv/ffcv). Its usage is a bit more involved since you need an additional dependency and you need to prepare the dataset, but you can make the data pipeline much faster with with it.\n",
    "- **augmentations matter**: try to train a model with and without augmentations.\n",
    "- **training stability and lr**: try to increase the learning rate (10x, 100x, ...). What happens to the learning curve? You can also try some learning rate scheduler.\n",
    "- **dropout**: as you know, dropout helps training DNN by regularizing the activations. But does it? You can try a small model such as a [SimpleMLP](https://avalanche-api.continualai.org/en/v0.3.1/generated/avalanche.models.SimpleMLP.html#avalanche.models.SimpleMLP). Does it always help? **hint**: as a general rule, if the model is too small (too shallow or not enough units), dropout will *decrease* the performance. This is a simple exercise to show you that scale matters when you make hyperparameter choices.\n",
    "- **add early stopping + model checkpointing**\n",
    "- **reproduce sota for CIFAR100**: use this notebook to reproduce a result from [this repo](https://github.com/weiaicunzai/pytorch-cifar100). NOTE: our resnet18 is slightly different from theirs (slim version, less units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6451ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "791aa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
