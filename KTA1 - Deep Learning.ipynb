{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a79a27e",
   "metadata": {},
   "source": [
    "# Deep Learning with PyTorch\n",
    "\n",
    "we will use:\n",
    "- PyTorch for the deep learning tools (modules, optimizers, losses, ...)\n",
    "- torchvision for the computer vision architectures and datasets\n",
    "- Avalanche for the multi-task streams of datasets and dynamic architectures (and later for incremental learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58f322",
   "metadata": {},
   "source": [
    "install dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79ac545",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install avalanche-lib==0.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7e729",
   "metadata": {},
   "source": [
    "tested with:\n",
    "- python 3.10\n",
    "- avalanche 0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad2b7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import avalanche\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9648a",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "first, we load the data.\n",
    "- The data is automatically downloaded the first time\n",
    "- `n_experiences` defines the lenght of the stream. Today we train our model offline, so we set `n_experiences=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b171887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitCIFAR10\n",
    "from avalanche.benchmarks.generators import benchmark_with_validation_stream\n",
    "\n",
    "benchmark = SplitCIFAR10(n_experiences=1)\n",
    "benchmark = benchmark_with_validation_stream(benchmark, validation_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67b4f21",
   "metadata": {},
   "source": [
    "we take the data from the streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c41404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = benchmark.train_stream[0].dataset\n",
    "valid_data = benchmark.valid_stream[0].dataset\n",
    "test_data = benchmark.test_stream[0].dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11708f64",
   "metadata": {},
   "source": [
    "NOTE: colors are wrong due to augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fbe4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label=6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_data[0][0].transpose(0,2).numpy())\n",
    "plt.title(f\"label={train_data[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b374b17",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e71f88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([100, 3, 32, 32])\n",
      "y: tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
      "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
      "        2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9,\n",
      "        6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6,\n",
      "        1, 1, 4, 1])\n",
      "t: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0])\n",
      "0, 100, 200, 300, "
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_size=100)\n",
    "for i, (x, y, t) in enumerate(dataloader):\n",
    "    # x is the input\n",
    "    # y is the target class\n",
    "    # t is the task label.\n",
    "    # We will use it for multi-task problems\n",
    "    pass\n",
    "    if i == 0:\n",
    "        print(f\"x.shape: {x.shape}\")\n",
    "        print(f\"y: {y}\")\n",
    "        print(f\"t: {t}\")\n",
    "    if i % 100 == 0:\n",
    "        print(i, end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218664fc",
   "metadata": {},
   "source": [
    "# Model\n",
    "We use a resnet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60eeb7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This is the slimmed ResNet as used by Lopez et al. in the GEM paper.\"\"\"\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu, avg_pool2d\n",
    "from avalanche.models import DynamicModule\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(0, len(sizes) - 1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            if i < (len(sizes) - 2):\n",
    "                layers.append(nn.ReLU())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_planes,\n",
    "                    self.expansion * planes,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.expansion * planes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes, nf):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = nf\n",
    "\n",
    "        self.conv1 = conv3x3(3, nf * 1)\n",
    "        self.bn1 = nn.BatchNorm2d(nf * 1)\n",
    "        self.layer1 = self._make_layer(block, nf * 1, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, nf * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, nf * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, nf * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(nf * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        bsz = x.size(0)\n",
    "        out = relu(self.bn1(self.conv1(x.view(bsz, 3, 32, 32))))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def SlimResNet18(nclasses, nf=20):\n",
    "    \"\"\"Slimmed ResNet18.\"\"\"\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], nclasses, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f67c27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(20, 40, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(40, 80, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(80, 160, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=160, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SlimResNet18(nclasses=100)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d5923",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b21d3fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5564: 100%|████████████████████████████████████████████████████████████████████| 1094/1094 [04:54<00:00,  3.72it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "device = 'cpu'  # do yourself a favor and use a gpu by setting device='cuda'\n",
    "model = SlimResNet18(nclasses=100)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Avalanche datasets have a training and eval mode (train/eval methods)\n",
    "# to activate different augmentations\n",
    "# don't forget to activate it!\n",
    "train_data = train_data.train()\n",
    "\n",
    "# Iterate over the dataset and train the model\n",
    "model.train()  # don't forget to set the training mode!\n",
    "for ep in range(10):\n",
    "    dataloader = DataLoader(train_data, batch_size=32)\n",
    "    pbar = tqdm(dataloader)\n",
    "    for (x, y, _) in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # the order matters!\n",
    "        # - reset the gradients\n",
    "        # - forward pass\n",
    "        # - backward pass\n",
    "        # - descent step\n",
    "        optimizer.zero_grad()   \n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pbar.set_description(f\"Loss: {loss.item():0.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a017e21f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f079156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ACC: 0.4688: 100%|█████████████████████████████████████████████████████████████████████| 1094/1094 [01:45<00:00, 10.39it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "train_data = valid_data.eval()\n",
    "\n",
    "dataloader = DataLoader(valid_data, batch_size=32)\n",
    "pbar = tqdm(dataloader)\n",
    "correct, tot = 0, 0\n",
    "for (x, y, _) in pbar:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    pred = model(x)\n",
    "    _, pred = torch.max(pred.data, 1)\n",
    "    correct += (pred == y).sum().item()\n",
    "    tot += x.shape[0]\n",
    "    pbar.set_description(f\"ACC: {correct / tot:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3281538b",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "Pick a couple of them to experiment with:\n",
    "\n",
    "- **resnet**: check the ResNet code. Do you understand everything? If not, check the documentation or ask for help.\n",
    "    - advanced version: now change the input transformations to change the input size of the images. Are you able to change the resnet architecture to make it compatible with the new image? Any change to the architecture is fine (just don't pad/crop the image...).\n",
    "- **dataloading**: If you have a large GPU (V100/A100), you need to feed the data fast enough, otherwise you are going to waste precious GPU cycles. Transformations can be expensive and dataloaders can make a big difference in performance.\n",
    "    - simple exercise: test and profile different augmentations from [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html). Play with the DataLoader parameters and measure its performance ([doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)). Try varying `num_workers`, `pin_memory`, and any other argument that you feel is relevant. Profile your code by timing the loading of the entire dataset in mini-batches, like you would do for a training epoch (without the forward/backward pass).\n",
    "    - advanced exercise: you can also try [FFCV](https://github.com/libffcv/ffcv). Its usage is a bit more involved since you need an additional dependency and you need to prepare the dataset, but you can make the data pipeline much faster with with it.\n",
    "- **augmentations matter**: try to train a model with and without augmentations.\n",
    "- **training stability and lr**: try to increase the learning rate (10x, 100x, ...). What happens to the learning curve? You can also try some learning rate scheduler.\n",
    "- **dropout**: as you know, dropout helps training DNN by regularizing the activations. But does it? You can try a small model such as a [SimpleMLP](https://avalanche-api.continualai.org/en/v0.3.1/generated/avalanche.models.SimpleMLP.html#avalanche.models.SimpleMLP). Does it always help? **hint**: as a general rule, if the model is too small (too shallow or not enough units), dropout will *decrease* the performance. This is a simple exercise to show you that scale matters when you make hyperparameter choices.\n",
    "- **add early stopping + model checkpointing**\n",
    "- **reproduce sota for CIFAR100**: use this notebook to reproduce a result from [this repo](https://github.com/weiaicunzai/pytorch-cifar100). NOTE: our resnet18 is slightly different from theirs (slim version, less units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6451ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
