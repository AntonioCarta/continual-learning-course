{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataloading, Memory Buffers, and Replay\n",
    "\n",
    "We are going to follow Avalanche implementation of replay methods. We need to define two components:\n",
    "- **Dataloaders** are used to provide balancing between groups (e.g. tasks/classes/experiences). This is especially useful when you have unbalanced data.\n",
    "- **Buffers** are used to store data from the previous experiences. They are dynamic datasets with a fixed maximum size, and they can be updated with new data continuously.\n",
    "\n",
    "A **Replay** method is a combination of a replay buffer and a custom dataloader.\n",
    "\n",
    "First, let's install Avalanche. You can skip this step if you have installed it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install avalanche-lib==0.3.1 pytorchcv==0.0.67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataloaders\n",
    "In Avalanche (and Pytorch) dataloaders are simple iterators, located under `avalanche.benchmarks.utils.data_loader`. Their interface is equivalent to pytorch's dataloaders. For example, `GroupBalancedDataLoader` takes a sequence of datasets and iterates over them by providing balanced mini-batches, where the number of samples is split equally among groups. Internally, it instantiate a `DataLoader` for each separate group. More specialized dataloaders exist such as `TaskBalancedDataLoader`.\n",
    "\n",
    "All the dataloaders accept keyword arguments (`**kwargs`) that are passed directly to the dataloaders for each group.\n",
    "\n",
    "We are going to use Avalanche dataloaders. You can check the code if you are curious. The bulk of the code is just setting the correct dimensions for the two batch sizes over time, depending on the hypeparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks import SplitMNIST\n",
    "from avalanche.benchmarks.utils.data_loader import GroupBalancedDataLoader\n",
    "benchmark = SplitMNIST(5, return_task_id=True)\n",
    "\n",
    "dl = GroupBalancedDataLoader([exp.dataset for exp in benchmark.train_stream], batch_size=5)\n",
    "for x, y, t in dl:\n",
    "    print(t.tolist())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Memory Buffers\n",
    "Memory buffers store data up to a maximum capacity, and they implement policies to select which data to store and which the to remove when the buffer is full. They are available in the module `avalanche.training.storage_policy`. The base class is the `ExemplarsBuffer`, which implements two methods:\n",
    "- `update(strategy)` - given the strategy's state it updates the buffer (using the data in `strategy.experience.dataset`).\n",
    "- `resize(strategy, new_size)` - updates the maximum size and updates the buffer accordingly.\n",
    "\n",
    "The data can be access using the attribute `buffer`.\n",
    "\n",
    "Here is a simple implementation of reservoir sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.training.storage_policy import ExemplarsBuffer\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "class ReservoirSamplingBuffer(ExemplarsBuffer):\n",
    "    \"\"\"Buffer updated with reservoir sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        :param max_size:\n",
    "        \"\"\"\n",
    "        # The algorithm follows\n",
    "        # https://en.wikipedia.org/wiki/Reservoir_sampling\n",
    "        # We sample a random uniform value in [0, 1] for each sample and\n",
    "        # choose the `size` samples with higher values.\n",
    "        # This is equivalent to a random selection of `size_samples`\n",
    "        # from the entire stream.\n",
    "        super().__init__(max_size)\n",
    "        # INVARIANT: _buffer_weights is always sorted.\n",
    "        self._buffer_weights = torch.zeros(0)\n",
    "\n",
    "    def update(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"Update buffer.\"\"\"\n",
    "        self.update_from_dataset(strategy.experience.dataset)\n",
    "\n",
    "    def update_from_dataset(self, new_data: AvalancheDataset):\n",
    "        \"\"\"Update the buffer using the given dataset.\n",
    "\n",
    "        :param new_data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        new_weights = torch.rand(len(new_data))\n",
    "        \n",
    "        # add new samples and sort them again\n",
    "        cat_weights = torch.cat([new_weights, self._buffer_weights])\n",
    "        cat_data = new_data.concat(self.buffer)\n",
    "        sorted_weights, sorted_idxs = cat_weights.sort(descending=True)\n",
    "        \n",
    "        # keep the top-k\n",
    "        buffer_idxs = sorted_idxs[: self.max_size]\n",
    "        self.buffer = cat_data.subset(buffer_idxs)\n",
    "        self._buffer_weights = sorted_weights[: self.max_size]\n",
    "\n",
    "    def resize(self, strategy, new_size):\n",
    "        \"\"\"Update the maximum size of the buffer.\"\"\"\n",
    "        self.max_size = new_size\n",
    "        if len(self.buffer) <= self.max_size:\n",
    "            return\n",
    "        self.buffer = classification_subset(\n",
    "            self.buffer, torch.arange(self.max_size)\n",
    "        )\n",
    "        self._buffer_weights = self._buffer_weights[: self.max_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max buffer size: 30, current size: 0\n"
     ]
    }
   ],
   "source": [
    "benchmark = SplitMNIST(5, return_task_id=False)\n",
    "storage_p = ReservoirSamplingBuffer(max_size=30)\n",
    "\n",
    "print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "At first, the buffer is empty. We can update it with data from a new experience.\n",
    "\n",
    "Notice that we use a `SimpleNamespace` because we want to use the buffer standalone, without instantiating an Avalanche strategy. Reservoir sampling requires only the `experience` from the strategy's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max buffer size: 30, current size: 30\n",
      "class targets: 7, 3, 7, 3, 3, 3, 7, 3, 7, 7, 7, 7, 7, 3, 7, 3, 7, 3, 3, 7, 3, 7, 3, 3, 3, 7, 7, 7, 7, 3\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 7, 3, 6, 6, 6, 6, 6, 0, 7, 6, 0, 0, 3, 0, 3, 3, 6, 7, 3, 7, 7, 6, 6, 0, 7, 7, 0, 7, 0, 3\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 7, 3, 6, 6, 6, 2, 6, 6, 1, 2, 0, 7, 6, 0, 2, 0, 3, 0, 2, 3, 3, 6, 7, 3, 2, 2, 7, 2, 7, 6\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 9, 7, 3, 6, 6, 9, 6, 4, 2, 6, 6, 1, 2, 0, 7, 6, 4, 0, 9, 2, 0, 4, 3, 4, 0, 2, 3, 3, 6, 9\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 9, 8, 7, 3, 6, 6, 5, 9, 6, 8, 8, 8, 4, 2, 6, 6, 8, 1, 2, 0, 7, 5, 6, 4, 0, 9, 8, 2, 0, 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    strategy_state = SimpleNamespace(experience=benchmark.train_stream[i])\n",
    "    storage_p.update(strategy_state)\n",
    "    targets = ', '.join([str(e) for e in storage_p.buffer.targets])\n",
    "    print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "    print(f\"class targets: {targets}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice after each update some samples are substituted with new data. Reservoir sampling select these samples randomly.\n",
    "\n",
    "Avalanche offers many more storage policies. For example, `ParametricBuffer` is a buffer split into several groups according to the `groupby` parameters (`None`, 'class', 'task', 'experience'), and according to an optional `ExemplarsSelectionStrategy` (random selection is the default choice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max buffer size: 30, current size: 0\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 3, 3, 3, 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 3, 3, 3, 3, 3, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 3, 3, 3, 3, 7, 7, 7, 0, 0, 0, 0, 6, 6, 6, 6, 1, 1, 1, 1, 2, 2, 2, 2, 4, 4, 4, 4, 9, 9, 9\n",
      "\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: 3, 3, 3, 7, 7, 7, 0, 0, 0, 6, 6, 6, 1, 1, 1, 2, 2, 2, 4, 4, 4, 9, 9, 9, 5, 5, 5, 8, 8, 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "storage_p = ParametricBuffer(\n",
    "    max_size=30,\n",
    "    groupby='class',\n",
    "    selection_strategy=RandomExemplarsSelectionStrategy()\n",
    ")\n",
    "\n",
    "print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "for i in range(5):\n",
    "    strategy_state = SimpleNamespace(experience=benchmark.train_stream[i])\n",
    "    storage_p.update(strategy_state)\n",
    "    targets = ', '.join([str(e) for e in storage_p.buffer.targets])\n",
    "    print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "    print(f\"class targets: {targets}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The advantage of using grouping buffers is that you get a balanced rehearsal buffer. You can even access the groups separately with the `buffer_groups` attribute. Combined with balanced dataloaders, you can ensure that the mini-batches stay balanced during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(group 3) -> size 3\n",
      "(group 7) -> size 3\n",
      "(group 0) -> size 3\n",
      "(group 6) -> size 3\n",
      "(group 1) -> size 3\n",
      "(group 2) -> size 3\n",
      "(group 4) -> size 3\n",
      "(group 9) -> size 3\n",
      "(group 5) -> size 3\n",
      "(group 8) -> size 3\n"
     ]
    }
   ],
   "source": [
    "for k, v in storage_p.buffer_groups.items():\n",
    "    print(f\"(group {k}) -> size {len(v.buffer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 3, 3, 7, 7, 7, 0, 0, 0, 6, 6, 6, 1, 1, 1, 2, 2, 2, 4, 4, 4, 9, 9, 9, 5, 5, 5, 8, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "datas = [v.buffer for v in storage_p.buffer_groups.values()]\n",
    "dl = GroupBalancedDataLoader(datas)\n",
    "\n",
    "for x, y, t in dl:\n",
    "    print(y.tolist())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max buffer size: 30, current size: 0\n",
      "Max buffer size: 30, current size: 0\n",
      "Experience (0)\n",
      "Train Epoch: 0 \tLoss: 0.242850\n",
      "Train Epoch: 1 \tLoss: 0.244739\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n",
      "\n",
      "Experience (1)\n",
      "Train Epoch: 0 \tLoss: 0.107958\n",
      "Train Epoch: 1 \tLoss: 0.067566\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: [5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "\n",
      "Experience (2)\n",
      "Train Epoch: 0 \tLoss: 0.083866\n",
      "Train Epoch: 1 \tLoss: 0.050811\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: [5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 8, 8, 8, 8, 8]\n",
      "\n",
      "Experience (3)\n",
      "Train Epoch: 0 \tLoss: 0.079434\n",
      "Train Epoch: 1 \tLoss: 0.030323\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: [5, 5, 5, 5, 6, 6, 6, 6, 1, 1, 1, 1, 2, 2, 2, 2, 0, 0, 0, 0, 8, 8, 8, 9, 9, 9, 3, 3, 3, 3]\n",
      "\n",
      "Experience (4)\n",
      "Train Epoch: 0 \tLoss: 0.136274\n",
      "Train Epoch: 1 \tLoss: 0.096130\n",
      "Max buffer size: 30, current size: 30\n",
      "class targets: [5, 5, 5, 6, 6, 6, 1, 1, 1, 2, 2, 2, 0, 0, 0, 8, 8, 8, 9, 9, 9, 3, 3, 3, 4, 4, 4, 7, 7, 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from avalanche.training.storage_policy import ParametricBuffer, RandomExemplarsSelectionStrategy\n",
    "from types import SimpleNamespace\n",
    "from avalanche.models import SimpleMLP\n",
    "from types import SimpleNamespace\n",
    "from avalanche.benchmarks.utils.data_loader import ReplayDataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models.utils import avalanche_model_adaptation\n",
    "from avalanche.models.dynamic_optimizers import reset_optimizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# scenario\n",
    "benchmark = SplitMNIST(\n",
    "    n_experiences=5,\n",
    "    return_task_id=False,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# model\n",
    "model = SimpleMLP()\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "device = 'cpu'\n",
    "num_epochs = 2\n",
    "\n",
    "# AVALANCHE: init replay buffer\n",
    "storage_p = ParametricBuffer(\n",
    "    max_size=30,\n",
    "    groupby='class',\n",
    "    selection_strategy=RandomExemplarsSelectionStrategy()\n",
    ")\n",
    "\n",
    "print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")   \n",
    "for exp in benchmark.train_stream:\n",
    "    print(f\"Experience ({exp.current_experience})\")\n",
    "    model.train()\n",
    "    avalanche_model_adaptation(model, exp)\n",
    "    reset_optimizer(optimizer, model)\n",
    "    dataset = exp.dataset\n",
    "    dataset = dataset.train()\n",
    " \n",
    "    for epoch in range(num_epochs):\n",
    "        # AVALANCHE: ReplayDataLoader to sample jointly from buffer and current data.\n",
    "        dl = ReplayDataLoader(dataset, storage_p.buffer, batch_size=128)\n",
    "        for x, y, t in dl:\n",
    "          x, y, t = x.to(device), y.to(device), t.to(device)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "          output = model(x)\n",
    "          loss = F.cross_entropy(output, y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "        print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, loss.item()))\n",
    "    \n",
    "    # AVALANCHE: you can use a SimpleNamespace if you want to use Avalanche components with your own code.\n",
    "    strategy_state = SimpleNamespace(experience=exp)\n",
    "    # AVALANCHE: update replay buffer\n",
    "    storage_p.update(strategy_state)\n",
    "    print(f\"Max buffer size: {storage_p.max_size}, current size: {len(storage_p.buffer)}\")\n",
    "    print(f\"class targets: {list(storage_p.buffer.targets)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "- try different memory sizes\n",
    "- write a custom dataloader that returns two minibatches, one from the memory, one from the current data. Add a coefficient `alpha` that controls the ratio between the memory and current loss such that `loss = curr_loss + alpha*memory_loss`.\n",
    "    - try different values of `alpha`\n",
    "    - try a linearly growing `alpha_t = alpha_base * t`, where t is the number of experiences seen up to now\n",
    "- implement GDumb. You can use the `ParametricBuffer` buffer to implement the class-balanced greedy sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
